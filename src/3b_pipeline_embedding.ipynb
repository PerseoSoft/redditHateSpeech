{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Neuronales\n",
    "\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importanci√≥n de librer√≠a requeridas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definici√≥n de variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_FILE_READ = 'docs/preprocessing_reddit_data.csv'\n",
    "TEXT_SAVE_FILE = 'docs/reddit_data_lda.csv'\n",
    "FILENAME_PICKLE = \"docs/tmpreddit.pickle\"\n",
    "n_clusters = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de los comentarios de Reddit\n",
    "\n",
    "Los comentarios fueron previamente preprocesados (Ver en TODO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(FILENAME_PICKLE, 'rb') as f:\n",
    "    df = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(df['lemma_tokens'])\n",
    "\n",
    "# Filtering Extremes\n",
    "id2word.filter_extremes(no_below=2, no_above=.99)\n",
    "\n",
    "# Creating a corpus object\n",
    "corpus = [id2word.doc2bow(d) for d in df['lemma_tokens']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_corpus = df['lemma_tokens']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=processed_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.train(processed_corpus, total_examples=len(processed_corpus), epochs=100)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = []\n",
    "vocabulary = list(model.wv.key_to_index)\n",
    "\n",
    "for key in model.wv.key_to_index:\n",
    "    word_vecs.append(model.wv[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jajajajjajaajaj', 0.9567767977714539),\n",
       " ('laconcho', 0.9120215177536011),\n",
       " ('bottle', 0.9049957394599915),\n",
       " ('branding', 0.8658552765846252),\n",
       " ('submarinar', 0.7980161905288696),\n",
       " ('golden', 0.7718115448951721),\n",
       " ('meanies', 0.7614740133285522),\n",
       " ('wormtongue', 0.6600823998451233),\n",
       " ('eyes', 0.6527519226074219),\n",
       " ('aragorn', 0.6500346064567566)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# algunas predicciones\n",
    "\n",
    "model.wv.most_similar(\"rucula\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generaci√≥n de vectores desde documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27791, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorize(list_of_docs, model):\n",
    "    \"\"\"Generate vectors for list of documents using a Word Embedding\n",
    "\n",
    "    Args:\n",
    "        list_of_docs: List of documents\n",
    "        model: Gensim's Word Embedding\n",
    "\n",
    "    Returns:\n",
    "        List of document vectors\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    for tokens in list_of_docs:\n",
    "        zero_vector = np.zeros(model.vector_size)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in model.wv:\n",
    "                try:\n",
    "                    vectors.append(model.wv[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            features.append(avg_vec)\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "    return features\n",
    "    \n",
    "vectorized_docs = vectorize(processed_corpus, model=model)\n",
    "len(vectorized_docs), len(vectorized_docs[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generaci√≥n de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def mbkmeans_clusters(\n",
    "\tX, \n",
    "    k, \n",
    "    mb, \n",
    "    print_silhouette_values, \n",
    "):\n",
    "    \"\"\"Generate clusters and print Silhouette metrics using MBKmeans\n",
    "\n",
    "    Args:\n",
    "        X: Matrix of features.\n",
    "        k: Number of clusters.\n",
    "        mb: Size of mini-batches.\n",
    "        print_silhouette_values: Print silhouette values per cluster.\n",
    "\n",
    "    Returns:\n",
    "        Trained clustering model and labels based on X.\n",
    "    \"\"\"\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=mb).fit(X)\n",
    "    print(f\"For n_clusters = {k}\")\n",
    "    print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n",
    "    print(f\"Inertia:{km.inertia_}\")\n",
    "\n",
    "    if print_silhouette_values:\n",
    "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
    "        print(f\"Silhouette values:\")\n",
    "        silhouette_values = []\n",
    "        for i in range(k):\n",
    "            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
    "            silhouette_values.append(\n",
    "                (\n",
    "                    i,\n",
    "                    cluster_silhouette_values.shape[0],\n",
    "                    cluster_silhouette_values.mean(),\n",
    "                    cluster_silhouette_values.min(),\n",
    "                    cluster_silhouette_values.max(),\n",
    "                )\n",
    "            )\n",
    "        silhouette_values = sorted(\n",
    "            silhouette_values, key=lambda tup: tup[2], reverse=True\n",
    "        )\n",
    "        for s in silhouette_values:\n",
    "            print(\n",
    "                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n",
    "            )\n",
    "    return km, km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 70\n",
      "Silhouette coefficient: 0.01\n",
      "Inertia:956541.2375425596\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-42f8d4a64bda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m clustering, cluster_labels = mbkmeans_clusters(\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectorized_docs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint_silhouette_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-337b1640bec3>\u001b[0m in \u001b[0;36mmbkmeans_clusters\u001b[0;34m(X, k, mb, print_silhouette_values)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprint_silhouette_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0msample_silhouette_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msilhouette_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Silhouette values:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0msilhouette_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/cluster/_unsupervised.py\u001b[0m in \u001b[0;36msilhouette_samples\u001b[0;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[1;32m    232\u001b[0m     reduce_func = functools.partial(_silhouette_reduce,\n\u001b[1;32m    233\u001b[0m                                     labels=labels, label_freqs=label_freqs)\n\u001b[0;32m--> 234\u001b[0;31m     results = zip(*pairwise_distances_chunked(X, reduce_func=reduce_func,\n\u001b[0m\u001b[1;32m    235\u001b[0m                                               **kwds))\n\u001b[1;32m    236\u001b[0m     \u001b[0mintra_clust_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minter_clust_dists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   1621\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m             \u001b[0mX_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1623\u001b[0;31m         D_chunk = pairwise_distances(X_chunk, Y, metric=metric,\n\u001b[0m\u001b[1;32m   1624\u001b[0m                                      n_jobs=n_jobs, **kwds)\n\u001b[1;32m   1625\u001b[0m         if ((X is Y or Y is None)\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0meffective_n_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m     \u001b[0;31m# enforce a threading backend to prevent data communication overhead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;31m# if dtype is already float64, no need to chunk and upcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mYY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clustering, cluster_labels = mbkmeans_clusters(\n",
    "\tX=vectorized_docs,\n",
    "    k=n_clusters,\n",
    "    mb=500,\n",
    "    print_silhouette_values=True,\n",
    ")\n",
    "df_clusters = pd.DataFrame({\n",
    "    \"text\": df[\"body\"].values,\n",
    "    \"tokens\": [\" \".join(text) for text in processed_corpus],\n",
    "    \"cluster\": cluster_labels\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Top terms* por cluster (basado en los centroides de los clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most representative terms per cluster (based on centroids):\n",
      "Cluster 0: amigo autorizacion jajsjsjj empija happn \n",
      "Cluster 1: andar bici sociales beiby zorzal \n",
      "Cluster 2: va catre enojarte desmechado amigoooo \n",
      "Cluster 3: vivir burbuja unicenter modestamente veiir \n",
      "Cluster 4: ‚°æ √©l ‚†ë ‚£á ‚†£ \n",
      "Cluster 5: pagar impuesto ganancia jejar cobrar \n",
      "Cluster 6: tomar mundo baso bombillar ‚†£ \n",
      "Cluster 7: precio regatear listado inflacion elefant \n",
      "Cluster 8: ‚°æ ‚†ë jsjajar cagar tiento \n",
      "Cluster 9: ver video ‚°æ guille aquino \n",
      "Cluster 10: mujer hombre desabrochar testosterona dalbon \n",
      "Cluster 11: dar sudar ‚†ë ‚°æ robartir \n",
      "Cluster 12: votar peronismo oficialismo voto perder \n",
      "Cluster 13: tipo capaz dirijar pelotudo ‚°æ \n",
      "Cluster 14: sexual depredador degenerados malo taaaan \n",
      "Cluster 15: ‚°æ atardecer avenger iluminano tortuoso \n",
      "Cluster 16: ‚°æ ‚†ë ballotage aniquilar atribuis \n",
      "Cluster 17: reboton dudamente noroeste 17695 escobas \n",
      "Cluster 18: vo segu√≠ espontaneo hegemon√≠a dec√≠s \n",
      "Cluster 19: grin drop exciting ape performing \n",
      "Cluster 20: tema bancate aforo intachable ling√º√≠stico \n",
      "Cluster 21: ‚°æ entender leer palabra hablar \n",
      "Cluster 22: negro apurate blanco mand√°melo pintamelo \n",
      "Cluster 23: gobierno ‚°æ \\*quilombo pudra \\*supermercado \n",
      "Cluster 24: milei xdxdxd bregmar javier rascare \n",
      "Cluster 25: ley mercantilismo reglamentaci√≥n tiltean regulaci√≥n \n",
      "Cluster 26: sacar serpiente war-boys renunci√°s mand√© \n",
      "Cluster 27: √∫nico problema funcionar thrall ‚°æ \n",
      "Cluster 28: viejo pediamo pesista decapitado feoy \n",
      "Cluster 29: estofado detox anchoar ensalado curry \n",
      "Cluster 30: hijo puta pobretonto kjjjjjjjjjjj pario \n",
      "Cluster 31: llegar punto ‚°æ ‚£á sobrefacturar \n",
      "Cluster 32: hacer dislocar fuselaje pantallito maloliente \n",
      "Cluster 33: quedar damon marmota ‚°æ saborido \n",
      "Cluster 34: dolar d√≥lar pesos peso d√≥lares \n",
      "Cluster 35: gente ‚°æ ‚†ë persona ‚†£ \n",
      "Cluster 36: producto limpiezo ofrecido buying pondrio \n",
      "Cluster 37: decir venir quer√©s ‚°æ siniestrar \n",
      "Cluster 38: seguro letrar cache pegajoso varela \n",
      "Cluster 39: ‚°æ mano arquitectura ‚†ë ‚£á \n",
      "Cluster 40: ‚°æ comentario post sub tweets \n",
      "Cluster 41: pensar ‚°æ opinar ‚†ë correas \n",
      "Cluster 42: \\-mr ofertar desensibilizar flequillir lamentar \n",
      "Cluster 43: tener duda mina boludo ‚°æ \n",
      "Cluster 44: ‚°æ jajjajjaaa pandemic ‚†ë ss \n",
      "Cluster 45: buscar follow presentamelar agregarla fundacion \n",
      "Cluster 46: salir biodegradar partitir chango cepo \n",
      "Cluster 47: espert ca√±o pegarl balar queeeir \n",
      "Cluster 48: lindo ‚°æ üòÄ hermoso felicitaci√≥n \n",
      "Cluster 49: x200b mirandar ÃãÃ≤peÕÄÃûdÕ†ÃïÕÇÕüÃûÕïiÃæÃÑÃãÕ°ÃûÃûÕâÃ´rÕûÕõÃÑÕÜÃ≤Ã¢ÃúÃ™ redacted lcdll \n",
      "Cluster 50: idea conc√≠s paralelismo medis tp \n",
      "Cluster 51: huevo presidente renuncie mk chup√≥ \n",
      "Cluster 52: foto mac√°n selfi salv√© serpiente \n",
      "Cluster 53: liebig peroodoncista doxe√≥ stretchly 2)esperar \n",
      "Cluster 54: argentino \\*votante am√©rica eficiencia horton \n",
      "Cluster 55: gracias conchito alegrar enhorabuena elocuencia \n",
      "Cluster 56: dejar subayudante fiche ruido sobrepasar \n",
      "Cluster 57: vida ‚°æ ‚†£ resolvemos buscarlos \n",
      "Cluster 58: querer ‚°æ ‚†£ ‚†ë obey \n",
      "Cluster 59: ley nacional trampo proyecto reglamentaci√≥n \n",
      "Cluster 60: argentina agedlikemilk europeo repu_arg pobreza \n",
      "Cluster 61: historia op crosspostea assburgers kudo \n",
      "Cluster 62: peso plastificado centavo comprar pesos \n",
      "Cluster 63: a√±o ‚°æ ‚£á semana ‚†ë \n",
      "Cluster 64: poner empaquetado ‚°ú sash persiga \n",
      "Cluster 65: ganar views idola procurador salme \n",
      "Cluster 66: ‚°æ ‚†ë ‚†£ pandemic hablandono \n",
      "Cluster 67: talking pandoro sub-familia ÌïúÏãùÏùÑ qlos \n",
      "Cluster 68: auto perro angustiado llevatelir perrito \n",
      "Cluster 69: gustar anderson wes filmografiar rasgo \n"
     ]
    }
   ],
   "source": [
    "print(\"Most representative terms per cluster (based on centroids):\")\n",
    "for i in range(n_clusters):\n",
    "    tokens_per_cluster = \"\"\n",
    "    most_representative = model.wv.most_similar(positive=[clustering.cluster_centers_[i]], topn=5)\n",
    "    for t in most_representative:\n",
    "        tokens_per_cluster += f\"{t[0]} \"\n",
    "    print(f\"Cluster {i}: {tokens_per_cluster}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Top terms* por cluster (basado en las palabras m√°s frecuentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: amigo(161) pasar(4) ten√©s(4) seguro(4) terminar(4) \n",
      "Cluster 1: andar(123) llamar(33) bici(18) √©l(9) ver(7) \n",
      "Cluster 2: va(205) pod(29) orto(16) √©l(13) vo(12) \n",
      "Cluster 3: vivir(203) pobre(18) gente(16) √©l(12) pa√≠s(11) \n",
      "Cluster 4: √©l(585) seguir(113) pasar(78) yo(73) vender(67) \n",
      "Cluster 5: pagar(224) impuesto(125) cobrar(53) sueldo(31) √©l(28) \n",
      "Cluster 6: tomar(183) mundo(68) mate(24) √©l(16) decisi√≥n(12) \n",
      "Cluster 7: precio(219) inflaci√≥n(40) d√≥lar(36) vender(32) bajar(31) \n",
      "Cluster 8: cagar(132) matar(74) risa(68) morir(61) √©l(45) \n",
      "Cluster 9: ver(378) video(105) √©l(50) cara(48) mirar(31) \n",
      "Cluster 10: mujer(124) hombre(112) gris(9) √©l(8) trans(8) \n",
      "Cluster 11: dar(241) √©l(89) vuelta(15) querer(12) yo(10) \n",
      "Cluster 12: peronismo(133) votar(131) voto(89) perder(89) elecci√≥n(32) \n",
      "Cluster 13: tipo(280) pelotudo(17) √©l(14) persona(13) pobre(10) \n",
      "Cluster 14: malo(63) sexual(19) so(11) arma(9) educaci√≥n(8) \n",
      "Cluster 15: ver(63) parecer(45) serie(44) √©l(38) pel√≠cula(32) \n",
      "Cluster 16: pol√≠tico(67) √©l(58) milei(55) partido(51) debate(46) \n",
      "Cluster 17: provincia(70) ciudad(49) zona(49) san(47) aires(41) \n",
      "Cluster 18: vo(230) decir(22) so(12) favor(12) pasar(10) \n",
      "Cluster 19: the(249) to(104) and(96) you(88) of(88) \n",
      "Cluster 20: tema(140) √©l(7) a√±o(5) gustar(5) escuchar(5) \n",
      "Cluster 21: hablar(191) entender(172) leer(90) √©l(40) palabra(34) \n",
      "Cluster 22: negro(98) blanco(38) ver(10) mercado(9) √©l(5) \n",
      "Cluster 23: gobierno(256) alberto(102) peronista(73) macri(71) √©l(52) \n",
      "Cluster 24: milei(187) espert(18) votar(14) √©l(11) decir(8) \n",
      "Cluster 25: ley(167) social(30) p√∫blico(29) √©l(21) pol√≠tico(18) \n",
      "Cluster 26: sacar(168) √©l(31) foto(22) voto(7) gente(7) \n",
      "Cluster 27: problema(103) √∫nico(88) funcionar(26) mundo(12) resto(8) \n",
      "Cluster 28: viejo(235) √©l(16) importar(10) a√±o(8) morir(8) \n",
      "Cluster 29: comer(108) carne(50) milanesa(38) rico(32) queso(30) \n",
      "Cluster 30: hijo(147) puta(80) madre(32) puto(27) padre(23) \n",
      "Cluster 31: llegar(137) punto(17) pasar(11) √©l(10) yo(5) \n",
      "Cluster 32: hacer(278) √©l(70) querer(12) paja(11) decir(11) \n",
      "Cluster 33: quedar(202) √©l(10) gente(8) decir(7) pensar(7) \n",
      "Cluster 34: dolar(76) peso(21) d√≥lar(13) pesos(10) subir(10) \n",
      "Cluster 35: gente(428) persona(128) √©l(55) pasar(36) creer(31) \n",
      "Cluster 36: producto(64) barato(28) comprar(23) vender(21) etiqueta(18) \n",
      "Cluster 37: decir(282) venir(210) √©l(28) a√±o(19) yo(18) \n",
      "Cluster 38: seguro(133) pelotudo(4) a√±o(4) votar(3) parecer(3) \n",
      "Cluster 39: mano(142) t√©n(77) caer(61) favor(39) pasar(39) \n",
      "Cluster 40: comentario(102) post(85) ac√°(82) sub(78) che(75) \n",
      "Cluster 41: pensar(281) √©l(24) pasar(12) gente(11) seguir(11) \n",
      "Cluster 42: √©l(59) se√±or(40) alto(28) falta(26) esperar(24) \n",
      "Cluster 43: tener(208) √©l(25) a√±o(15) duda(6) comprar(6) \n",
      "Cluster 44: √©l(87) calor(81) tirar(70) casa(62) agua(55) \n",
      "Cluster 45: buscar(166) nombre(76) dato(37) dni(24) √©l(16) \n",
      "Cluster 46: salir(287) calle(13) gente(10) casa(10) √©l(9) \n",
      "Cluster 47: espert(132) ca√±o(59) debate(28) milei(19) √©l(16) \n",
      "Cluster 48: lindo(138) hermoso(23) amor(16) feliz(15) √©l(13) \n",
      "Cluster 49: x200b(168) /s(54) ah(26) faltar(23) don(21) \n",
      "Cluster 50: idea(167) ver(10) pasar(8) persona(7) √©l(6) \n",
      "Cluster 51: huevo(16) presidente(2) vendo(1) barato(1) qued(1) \n",
      "Cluster 52: foto(164) ver(14) subir(8) gana(7) auto(7) \n",
      "Cluster 53: depender(29) empresa(21) afip(21) +(20) curso(17) \n",
      "Cluster 54: argentino(249) pasar(11) naci√≥n(8) rep√∫blica(6) entender(6) \n",
      "Cluster 55: gracias(160) che(7) √©l(7) compartir(7) jaja(6) \n",
      "Cluster 56: dejar(282) √©l(28) pasar(17) entrar(9) preguntar(7) \n",
      "Cluster 57: vida(277) √©l(22) seguir(13) pasar(12) disfrutar(11) \n",
      "Cluster 58: querer(297) √©l(20) yo(10) seguir(9) pasar(8) \n",
      "Cluster 59: ley(51) nacional(36) kimchi(5) etiquetado(4) importante(4) \n",
      "Cluster 60: argentina(217) pa√≠s(16) pensar(8) mundo(8) r(8) \n",
      "Cluster 61: historia(44) op(34) triste(7) contar(7) paso(5) \n",
      "Cluster 62: comprar(168) peso(88) pesos(55) billete(52) mes(43) \n",
      "Cluster 63: a√±o(351) pasar(175) semana(88) volver(74) mes(64) \n",
      "Cluster 64: poner(262) √©l(41) pasar(9) gente(7) az√∫car(6) \n",
      "Cluster 65: ganar(182) elecci√≥n(14) perder(13) √©l(9) voto(8) \n",
      "Cluster 66: pa√≠s(266) argentino(78) √©l(67) ten√©s(52) a√±o(47) \n",
      "Cluster 67: ‚†Ä(685) Ô∏è(21) √©l(19) of(18) üòÉ(15) \n",
      "Cluster 68: auto(126) perro(107) llevar(26) √©l(22) calle(22) \n",
      "Cluster 69: gustar(154) calor(8) arte(5) alberto(4) gusto(4) \n"
     ]
    }
   ],
   "source": [
    "for i in range(n_clusters):\n",
    "    tokens_per_cluster = \"\"\n",
    "    most_frequent = Counter(\" \".join(df_clusters.query(f\"cluster == {i}\")[\"tokens\"]).split()).most_common(5)\n",
    "    for t in most_frequent:\n",
    "        tokens_per_cluster += f\"{t[0]}({str(t[1])}) \"\n",
    "    print(f\"Cluster {i}: {tokens_per_cluster}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recupere los documentos m√°s representativos (basados en los centroides de los cl√∫steres) para un cluster en particular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tema es que no soy comerciante. Nisiquiera tengo cuit.\n",
      "-------------\n",
      "Eso es otro tema que engloba otras t√°cticas complejas de exponer.\n",
      "-------------\n",
      "Ciro Pertusi tiene un historial MUY largo con tema pedofilia, no es solo esa letra.\n",
      "-------------\n",
      "Bueno pero como abordas temas como el aborto adonde necesariamente se proh√≠be desde el estado?\n",
      "-------------\n",
      "No es un tema prioritario pero quiere hacer un plebiscito?\n",
      "-------------\n",
      "\"... hay publicaciones cient√≠ficas del tema\".. Fuente: Arial 12\n",
      "-------------\n",
      "Hay que apurar el tema de eutanasia.\n",
      "-------------\n",
      "Que temas escuchaste? Yo lo conoci por Bancate ese defecto, y me volo la cabeza ese tema\n",
      "-------------\n",
      "Por ejemplo, la parte de promoci√≥n/publicidad es un quilombo y el tema de packaging es otra cagada. Perd√≥name pero me parece muy limitado el trabajo que se hizo para ser algo que tiene a√±os.\n",
      "-------------\n",
      "M√°s o menos, Les Luthiers no era stand up. Hab√≠a mucho tema de m√∫sica y el humor era, en su mayor√≠a, ling√º√≠stico\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "test_cluster = 20\n",
    "most_representative_docs = np.argsort(\n",
    "    np.linalg.norm(vectorized_docs - clustering.cluster_centers_[test_cluster], axis=1)\n",
    ")\n",
    "for d in most_representative_docs[:10]:\n",
    "    print( df[\"body\"].values[d])\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print(len(vectorized_docs))\n",
    "#print(vectorized_docs[0])\n",
    "\n",
    "test_v = vectorize([['defender', 'peso', 'siente', 'coraz√≥n', 'compro', 'pesos', 'tasa', 'fijo', 'a√±o']], model=model)\n",
    "prediction = clustering.predict(test_v)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = pd.read_csv(TEXT_FILE_READ)\n",
    "\n",
    "def get_cluster(row):\n",
    "    test_v = vectorize([row], model=model)\n",
    "    return clustering.predict(test_v)\n",
    "\n",
    "reddit['cluster'] = reddit.apply(lambda row: get_cluster(row['lemma_tokens']) , axis = 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>flair</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>body</th>\n",
       "      <th>comment_parent_id</th>\n",
       "      <th>is_replay</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>Unnamed: 12</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "      <th>lemma_tokens</th>\n",
       "      <th>body_preprocessing</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>hfw14mt</td>\n",
       "      <td>Discusionüßê</td>\n",
       "      <td>1</td>\n",
       "      <td>todo para decir que tapaste el ba√±o. tira un b...</td>\n",
       "      <td>q44kw3</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['tapastir', 'ba√±o', 'tirar', 'balde', 'aguo']</td>\n",
       "      <td>tapastir ba√±o tirar balde aguo</td>\n",
       "      <td>[67]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>hfw41eh</td>\n",
       "      <td>Discusionüßê</td>\n",
       "      <td>0</td>\n",
       "      <td>sopapa primero master, si hay tap√≥n te vas a t...</td>\n",
       "      <td>hfw14mt</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['sopapa', 'master', 'tap√≥n', 'va', 'te√±ir', '...</td>\n",
       "      <td>sopapa master tap√≥n va te√±ir medio</td>\n",
       "      <td>[67]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>hfw1ao2</td>\n",
       "      <td>Discusionüßê</td>\n",
       "      <td>0</td>\n",
       "      <td>Usas la sopapa, o tiras agua caliente con un b...</td>\n",
       "      <td>q44kw3</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['sopapo', 'tira', 'agua', 'caliente', 'balde']</td>\n",
       "      <td>sopapo tira agua caliente balde</td>\n",
       "      <td>[67]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>hfw3jof</td>\n",
       "      <td>Discusionüßê</td>\n",
       "      <td>2</td>\n",
       "      <td>Lo que he probado que siempre me dio resultado...</td>\n",
       "      <td>q44kw3</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['probado', 'resultado', 'sellar', 'boca', 'in...</td>\n",
       "      <td>probado resultado sellar boca inodoro tirar ca...</td>\n",
       "      <td>[67]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>hfw6v4i</td>\n",
       "      <td>Discusionüßê</td>\n",
       "      <td>0</td>\n",
       "      <td>Estas cobrando por dar mantenimiento y no sabe...</td>\n",
       "      <td>q44kw3</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['cobrar', 'mantenimiento', 'carajo', 'kjjjjjj...</td>\n",
       "      <td>cobrar mantenimiento carajo kjjjjjjjjj vivirio...</td>\n",
       "      <td>[67]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>hfw26iv</td>\n",
       "      <td>Discusionüßê</td>\n",
       "      <td>0</td>\n",
       "      <td>Si tenes algo con punta, metelo y hace un poco...</td>\n",
       "      <td>q44kw3</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['t√©n', 'punto', 'metelo', 'fuerza', 'romper',...</td>\n",
       "      <td>t√©n punto metelo fuerza romper tapo ba√±o tirar...</td>\n",
       "      <td>[67]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>hfw2gof</td>\n",
       "      <td>Discusionüßê</td>\n",
       "      <td>1</td>\n",
       "      <td>Con una manguera para regar el jard√≠n, si tene...</td>\n",
       "      <td>q44kw3</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['regar', 'jard√≠n', 't√©n', 'pod', 'probar']</td>\n",
       "      <td>regar jard√≠n t√©n pod probar</td>\n",
       "      <td>[67]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>hfw5s13</td>\n",
       "      <td>Discusionüßê</td>\n",
       "      <td>0</td>\n",
       "      <td>despues regas el jardin y se lava sola, solo q...</td>\n",
       "      <td>hfw2gof</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['rega', 'jardin', 'lava', 'ten√©s', 'lavarte',...</td>\n",
       "      <td>rega jardin lava ten√©s lavarte mano pulgar chorro</td>\n",
       "      <td>[67]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>hfw3air</td>\n",
       "      <td>Discusionüßê</td>\n",
       "      <td>0</td>\n",
       "      <td>La respuesta real es que se venden unos ca√±os ...</td>\n",
       "      <td>q44kw3</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['respuesta', 'real', 'vender', 'ca√±o', 'alamb...</td>\n",
       "      <td>respuesta real vender ca√±o alambrado decir ca√±...</td>\n",
       "      <td>[67]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>hfvxa6w</td>\n",
       "      <td>Discusionüßê</td>\n",
       "      <td>3</td>\n",
       "      <td>Mi alfajor favorito es el Havana</td>\n",
       "      <td>q443eo</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['alfajor', 'favorito', 'hav√°n']</td>\n",
       "      <td>alfajor favorito hav√°n</td>\n",
       "      <td>[67]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score       id       flair  comms_num  \\\n",
       "0      1  hfw14mt  Discusionüßê          1   \n",
       "1      1  hfw41eh  Discusionüßê          0   \n",
       "2      1  hfw1ao2  Discusionüßê          0   \n",
       "3      1  hfw3jof  Discusionüßê          2   \n",
       "4      1  hfw6v4i  Discusionüßê          0   \n",
       "5      1  hfw26iv  Discusionüßê          0   \n",
       "6      1  hfw2gof  Discusionüßê          1   \n",
       "7      1  hfw5s13  Discusionüßê          0   \n",
       "8      1  hfw3air  Discusionüßê          0   \n",
       "9      7  hfvxa6w  Discusionüßê          3   \n",
       "\n",
       "                                                body comment_parent_id  \\\n",
       "0  todo para decir que tapaste el ba√±o. tira un b...            q44kw3   \n",
       "1  sopapa primero master, si hay tap√≥n te vas a t...           hfw14mt   \n",
       "2  Usas la sopapa, o tiras agua caliente con un b...            q44kw3   \n",
       "3  Lo que he probado que siempre me dio resultado...            q44kw3   \n",
       "4  Estas cobrando por dar mantenimiento y no sabe...            q44kw3   \n",
       "5  Si tenes algo con punta, metelo y hace un poco...            q44kw3   \n",
       "6  Con una manguera para regar el jard√≠n, si tene...            q44kw3   \n",
       "7  despues regas el jardin y se lava sola, solo q...           hfw2gof   \n",
       "8  La respuesta real es que se venden unos ca√±os ...            q44kw3   \n",
       "9                   Mi alfajor favorito es el Havana            q443eo   \n",
       "\n",
       "  is_replay Unnamed: 7 Unnamed: 8 Unnamed: 9 Unnamed: 10 Unnamed: 11  \\\n",
       "0     False        NaN        NaN        NaN         NaN         NaN   \n",
       "1      True        NaN        NaN        NaN         NaN         NaN   \n",
       "2     False        NaN        NaN        NaN         NaN         NaN   \n",
       "3     False        NaN        NaN        NaN         NaN         NaN   \n",
       "4     False        NaN        NaN        NaN         NaN         NaN   \n",
       "5     False        NaN        NaN        NaN         NaN         NaN   \n",
       "6     False        NaN        NaN        NaN         NaN         NaN   \n",
       "7      True        NaN        NaN        NaN         NaN         NaN   \n",
       "8     False        NaN        NaN        NaN         NaN         NaN   \n",
       "9     False        NaN        NaN        NaN         NaN         NaN   \n",
       "\n",
       "  Unnamed: 12 Unnamed: 13 Unnamed: 14  \\\n",
       "0         NaN         NaN         NaN   \n",
       "1         NaN         NaN         NaN   \n",
       "2         NaN         NaN         NaN   \n",
       "3         NaN         NaN         NaN   \n",
       "4         NaN         NaN         NaN   \n",
       "5         NaN         NaN         NaN   \n",
       "6         NaN         NaN         NaN   \n",
       "7         NaN         NaN         NaN   \n",
       "8         NaN         NaN         NaN   \n",
       "9         NaN         NaN         NaN   \n",
       "\n",
       "                                        lemma_tokens  \\\n",
       "0     ['tapastir', 'ba√±o', 'tirar', 'balde', 'aguo']   \n",
       "1  ['sopapa', 'master', 'tap√≥n', 'va', 'te√±ir', '...   \n",
       "2    ['sopapo', 'tira', 'agua', 'caliente', 'balde']   \n",
       "3  ['probado', 'resultado', 'sellar', 'boca', 'in...   \n",
       "4  ['cobrar', 'mantenimiento', 'carajo', 'kjjjjjj...   \n",
       "5  ['t√©n', 'punto', 'metelo', 'fuerza', 'romper',...   \n",
       "6        ['regar', 'jard√≠n', 't√©n', 'pod', 'probar']   \n",
       "7  ['rega', 'jardin', 'lava', 'ten√©s', 'lavarte',...   \n",
       "8  ['respuesta', 'real', 'vender', 'ca√±o', 'alamb...   \n",
       "9                   ['alfajor', 'favorito', 'hav√°n']   \n",
       "\n",
       "                                  body_preprocessing cluster  \n",
       "0                     tapastir ba√±o tirar balde aguo    [67]  \n",
       "1                 sopapa master tap√≥n va te√±ir medio    [67]  \n",
       "2                    sopapo tira agua caliente balde    [67]  \n",
       "3  probado resultado sellar boca inodoro tirar ca...    [67]  \n",
       "4  cobrar mantenimiento carajo kjjjjjjjjj vivirio...    [67]  \n",
       "5  t√©n punto metelo fuerza romper tapo ba√±o tirar...    [67]  \n",
       "6                        regar jard√≠n t√©n pod probar    [67]  \n",
       "7  rega jardin lava ten√©s lavarte mano pulgar chorro    [67]  \n",
       "8  respuesta real vender ca√±o alambrado decir ca√±...    [67]  \n",
       "9                             alfajor favorito hav√°n    [67]  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show\n",
    "reddit.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit.to_csv(TEXT_SAVE_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_clusters):\n",
    "    reddit[(reddit[\"cluster\"] == i)][['flair', 'body']].to_csv('docs/testlda/' + str(i) + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
