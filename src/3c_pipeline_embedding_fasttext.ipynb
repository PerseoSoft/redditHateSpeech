{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Neuronales\n",
    "\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importanci√≥n de librer√≠a requeridas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definici√≥n de variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_FILE_READ = 'docs/preprocessing_reddit_data.csv'\n",
    "TEXT_SAVE_FILE = 'docs/reddit_data_fasttext.csv'\n",
    "FILENAME_PICKLE = \"docs/tmpreddit.pickle\"\n",
    "n_clusters = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de los comentarios de Reddit\n",
    "\n",
    "Los comentarios fueron previamente preprocesados (Ver en TODO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(FILENAME_PICKLE, 'rb') as f:\n",
    "    df = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(df['lemma_tokens'])\n",
    "\n",
    "# Filtering Extremes\n",
    "id2word.filter_extremes(no_below=2, no_above=.99)\n",
    "\n",
    "# Creating a corpus object\n",
    "corpus = [id2word.doc2bow(d) for d in df['lemma_tokens']]\n",
    "\n",
    "processed_corpus = df['lemma_tokens']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18381159, 18695800)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FastText(sentences=processed_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.train(processed_corpus, total_examples=len(processed_corpus), epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algunas predicciones\n",
    "model.get_nearest_neighbors('peron')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9955493211746216, 'cristi'),\n",
       " (0.9921181797981262, 'kirchnerista'),\n",
       " (0.9887705445289612, 'cristina_fernandez'),\n",
       " (0.9884024858474731, 'cristina_fern√°ndez'),\n",
       " (0.988014280796051, 'crist√≠n'),\n",
       " (0.9856591820716858, 'kirchnerismo'),\n",
       " (0.9838628172874451, 'fern√°ndez'),\n",
       " (0.9806126952171326, 'argentino\"'),\n",
       " (0.9791547060012817, 'turista'),\n",
       " (0.9788342118263245, 'ladrona')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_nearest_neighbors('cristina')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9962456822395325, 'cfk'),\n",
       " (0.9956827163696289, 'lopez'),\n",
       " (0.9956071376800537, 'seinfeld'),\n",
       " (0.9955170154571533, 'zombie'),\n",
       " (0.9954903721809387, 'remake'),\n",
       " (0.9953209161758423, 'bullrich'),\n",
       " (0.9948703646659851, 'stealth'),\n",
       " (0.9945695400238037, 'users'),\n",
       " (0.9945260286331177, 'stewart'),\n",
       " (0.9945029616355896, 'evil')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_nearest_neighbors('nestor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9928846955299377, 'economy'),\n",
       " (0.9909766316413879, 'neoliberalismo'),\n",
       " (0.9904403686523438, 'nac√≠'),\n",
       " (0.9893696904182434, 'republica_argentina'),\n",
       " (0.9881485104560852, 'ladron'),\n",
       " (0.9877192378044128, 'fernandez'),\n",
       " (0.9875622391700745, 'crea'),\n",
       " (0.986701488494873, 'arg'),\n",
       " (0.9864534735679626, 'referis'),\n",
       " (0.9864226579666138, 'cereal')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_nearest_neighbors('neoliberal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9982995390892029, 'doctrina'),\n",
       " (0.9981448650360107, 'petr√≥leo'),\n",
       " (0.9979062676429749, 'turbina'),\n",
       " (0.9976403117179871, 'juez'),\n",
       " (0.9974097609519958, 'marihu√°n'),\n",
       " (0.9973348379135132, 'gris'),\n",
       " (0.9971646070480347, 'nena'),\n",
       " (0.9970555901527405, 'wanda_nara'),\n",
       " (0.9969688057899475, 'gorila'),\n",
       " (0.9969642758369446, 'pel√≠cula')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_nearest_neighbors('malvinas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9841022491455078, 'malvina'),\n",
       " (0.9838746190071106, 't√≠terir'),\n",
       " (0.9836748242378235, 'dona'),\n",
       " (0.9829391837120056, 'malvinas'),\n",
       " (0.9824694395065308, 'doctrina'),\n",
       " (0.9824132919311523, 'estatua'),\n",
       " (0.9818088412284851, 'reina'),\n",
       " (0.9814509153366089, 'infarto'),\n",
       " (0.9808228015899658, 'estafa'),\n",
       " (0.980640709400177, 'archivo')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_analogies('cristina', 'ladrona', 'alberto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mapuchez', 0.975992739200592),\n",
       " ('mapuches', 0.9645689725875854),\n",
       " ('mapuchecoin', 0.9489633440971375),\n",
       " ('mapuch', 0.9372188448905945),\n",
       " ('no-mapuches', 0.9177072048187256),\n",
       " ('pseudomapuche', 0.9131770133972168),\n",
       " ('mapu', 0.8864879012107849),\n",
       " ('mapuchir', 0.8846395611763),\n",
       " ('pseudo-mapuches', 0.8769819736480713),\n",
       " ('pseudomapuch', 0.8387230038642883)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('mapuche')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generaci√≥n de vectores desde documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27791, 100)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorize(list_of_docs, model):\n",
    "    \"\"\"Generate vectors for list of documents using a Word Embedding\n",
    "\n",
    "    Args:\n",
    "        list_of_docs: List of documents\n",
    "        model: Gensim's Word Embedding\n",
    "\n",
    "    Returns:\n",
    "        List of document vectors\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    for tokens in list_of_docs:\n",
    "        zero_vector = np.zeros(model.vector_size)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in model.wv:\n",
    "                try:\n",
    "                    vectors.append(model.wv[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            features.append(avg_vec)\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "    return features\n",
    "    \n",
    "vectorized_docs = vectorize(processed_corpus, model=model)\n",
    "len(vectorized_docs), len(vectorized_docs[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generaci√≥n de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def mbkmeans_clusters(\n",
    "\tX, \n",
    "    k, \n",
    "    mb, \n",
    "    print_silhouette_values, \n",
    "):\n",
    "    \"\"\"Generate clusters and print Silhouette metrics using MBKmeans\n",
    "\n",
    "    Args:\n",
    "        X: Matrix of features.\n",
    "        k: Number of clusters.\n",
    "        mb: Size of mini-batches.\n",
    "        print_silhouette_values: Print silhouette values per cluster.\n",
    "\n",
    "    Returns:\n",
    "        Trained clustering model and labels based on X.\n",
    "    \"\"\"\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=mb).fit(X)\n",
    "    print(f\"For n_clusters = {k}\")\n",
    "    print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n",
    "    print(f\"Inertia:{km.inertia_}\")\n",
    "\n",
    "    if print_silhouette_values:\n",
    "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
    "        print(f\"Silhouette values:\")\n",
    "        silhouette_values = []\n",
    "        for i in range(k):\n",
    "            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
    "            silhouette_values.append(\n",
    "                (\n",
    "                    i,\n",
    "                    cluster_silhouette_values.shape[0],\n",
    "                    cluster_silhouette_values.mean(),\n",
    "                    cluster_silhouette_values.min(),\n",
    "                    cluster_silhouette_values.max(),\n",
    "                )\n",
    "            )\n",
    "        silhouette_values = sorted(\n",
    "            silhouette_values, key=lambda tup: tup[2], reverse=True\n",
    "        )\n",
    "        for s in silhouette_values:\n",
    "            print(\n",
    "                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n",
    "            )\n",
    "    return km, km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 70\n",
      "Silhouette coefficient: 0.01\n",
      "Inertia:1635970.9632375669\n",
      "Silhouette values:\n",
      "    Cluster 13: Size:223 | Avg:0.22 | Min:0.07 | Max: 0.38\n",
      "    Cluster 49: Size:156 | Avg:0.14 | Min:-0.05 | Max: 0.31\n",
      "    Cluster 67: Size:53 | Avg:0.11 | Min:-0.07 | Max: 0.34\n",
      "    Cluster 11: Size:349 | Avg:0.11 | Min:-0.05 | Max: 0.27\n",
      "    Cluster 28: Size:236 | Avg:0.09 | Min:-0.06 | Max: 0.26\n",
      "    Cluster 21: Size:139 | Avg:0.09 | Min:-0.07 | Max: 0.27\n",
      "    Cluster 42: Size:90 | Avg:0.09 | Min:-0.08 | Max: 0.30\n",
      "    Cluster 26: Size:189 | Avg:0.08 | Min:-0.08 | Max: 0.29\n",
      "    Cluster 63: Size:303 | Avg:0.08 | Min:-0.05 | Max: 0.26\n",
      "    Cluster 27: Size:90 | Avg:0.07 | Min:-0.06 | Max: 0.26\n",
      "    Cluster 45: Size:207 | Avg:0.07 | Min:-0.09 | Max: 0.26\n",
      "    Cluster 23: Size:207 | Avg:0.07 | Min:-0.07 | Max: 0.28\n",
      "    Cluster 34: Size:92 | Avg:0.07 | Min:-0.05 | Max: 0.23\n",
      "    Cluster 33: Size:256 | Avg:0.06 | Min:-0.08 | Max: 0.24\n",
      "    Cluster 38: Size:231 | Avg:0.05 | Min:-0.07 | Max: 0.26\n",
      "    Cluster 20: Size:1648 | Avg:0.05 | Min:0.01 | Max: 0.11\n",
      "    Cluster 10: Size:178 | Avg:0.05 | Min:-0.07 | Max: 0.23\n",
      "    Cluster 58: Size:201 | Avg:0.05 | Min:-0.08 | Max: 0.25\n",
      "    Cluster 50: Size:358 | Avg:0.05 | Min:-0.14 | Max: 0.17\n",
      "    Cluster 66: Size:371 | Avg:0.05 | Min:-0.07 | Max: 0.23\n",
      "    Cluster 64: Size:272 | Avg:0.05 | Min:-0.09 | Max: 0.21\n",
      "    Cluster 14: Size:112 | Avg:0.04 | Min:-0.14 | Max: 0.27\n",
      "    Cluster 59: Size:160 | Avg:0.04 | Min:-0.10 | Max: 0.23\n",
      "    Cluster 48: Size:123 | Avg:0.04 | Min:-0.08 | Max: 0.26\n",
      "    Cluster 5: Size:387 | Avg:0.03 | Min:-0.08 | Max: 0.19\n",
      "    Cluster 65: Size:800 | Avg:0.03 | Min:-0.04 | Max: 0.18\n",
      "    Cluster 25: Size:468 | Avg:0.03 | Min:-0.11 | Max: 0.25\n",
      "    Cluster 43: Size:195 | Avg:0.03 | Min:-0.10 | Max: 0.21\n",
      "    Cluster 22: Size:958 | Avg:0.03 | Min:-0.02 | Max: 0.08\n",
      "    Cluster 40: Size:256 | Avg:0.03 | Min:-0.10 | Max: 0.21\n",
      "    Cluster 47: Size:93 | Avg:0.03 | Min:-0.12 | Max: 0.24\n",
      "    Cluster 19: Size:157 | Avg:0.02 | Min:-0.13 | Max: 0.24\n",
      "    Cluster 53: Size:284 | Avg:0.02 | Min:-0.08 | Max: 0.19\n",
      "    Cluster 9: Size:347 | Avg:0.02 | Min:-0.13 | Max: 0.20\n",
      "    Cluster 52: Size:285 | Avg:0.01 | Min:-0.11 | Max: 0.18\n",
      "    Cluster 24: Size:428 | Avg:0.01 | Min:-0.11 | Max: 0.22\n",
      "    Cluster 69: Size:146 | Avg:0.01 | Min:-0.15 | Max: 0.23\n",
      "    Cluster 12: Size:996 | Avg:0.01 | Min:-0.04 | Max: 0.08\n",
      "    Cluster 62: Size:434 | Avg:0.00 | Min:-0.11 | Max: 0.19\n",
      "    Cluster 7: Size:685 | Avg:-0.00 | Min:-0.07 | Max: 0.11\n",
      "    Cluster 18: Size:1480 | Avg:-0.01 | Min:-0.07 | Max: 0.07\n",
      "    Cluster 30: Size:128 | Avg:-0.01 | Min:-0.14 | Max: 0.16\n",
      "    Cluster 16: Size:1236 | Avg:-0.01 | Min:-0.07 | Max: 0.07\n",
      "    Cluster 54: Size:319 | Avg:-0.01 | Min:-0.12 | Max: 0.15\n",
      "    Cluster 55: Size:349 | Avg:-0.01 | Min:-0.12 | Max: 0.15\n",
      "    Cluster 56: Size:250 | Avg:-0.01 | Min:-0.15 | Max: 0.14\n",
      "    Cluster 29: Size:519 | Avg:-0.01 | Min:-0.13 | Max: 0.14\n",
      "    Cluster 57: Size:711 | Avg:-0.02 | Min:-0.08 | Max: 0.08\n",
      "    Cluster 36: Size:570 | Avg:-0.02 | Min:-0.13 | Max: 0.15\n",
      "    Cluster 61: Size:762 | Avg:-0.02 | Min:-0.10 | Max: 0.10\n",
      "    Cluster 32: Size:237 | Avg:-0.02 | Min:-0.15 | Max: 0.13\n",
      "    Cluster 46: Size:1016 | Avg:-0.02 | Min:-0.10 | Max: 0.08\n",
      "    Cluster 51: Size:242 | Avg:-0.02 | Min:-0.14 | Max: 0.19\n",
      "    Cluster 3: Size:429 | Avg:-0.02 | Min:-0.13 | Max: 0.15\n",
      "    Cluster 68: Size:285 | Avg:-0.02 | Min:-0.10 | Max: 0.11\n",
      "    Cluster 31: Size:381 | Avg:-0.03 | Min:-0.14 | Max: 0.15\n",
      "    Cluster 39: Size:922 | Avg:-0.03 | Min:-0.10 | Max: 0.09\n",
      "    Cluster 8: Size:888 | Avg:-0.03 | Min:-0.12 | Max: 0.09\n",
      "    Cluster 17: Size:191 | Avg:-0.03 | Min:-0.16 | Max: 0.15\n",
      "    Cluster 41: Size:137 | Avg:-0.03 | Min:-0.13 | Max: 0.14\n",
      "    Cluster 15: Size:227 | Avg:-0.03 | Min:-0.14 | Max: 0.14\n",
      "    Cluster 6: Size:789 | Avg:-0.04 | Min:-0.13 | Max: 0.10\n",
      "    Cluster 44: Size:120 | Avg:-0.04 | Min:-0.12 | Max: 0.10\n",
      "    Cluster 2: Size:462 | Avg:-0.04 | Min:-0.11 | Max: 0.09\n",
      "    Cluster 0: Size:239 | Avg:-0.04 | Min:-0.15 | Max: 0.10\n",
      "    Cluster 37: Size:181 | Avg:-0.05 | Min:-0.19 | Max: 0.13\n",
      "    Cluster 1: Size:528 | Avg:-0.05 | Min:-0.15 | Max: 0.07\n",
      "    Cluster 60: Size:373 | Avg:-0.05 | Min:-0.19 | Max: 0.14\n",
      "    Cluster 35: Size:247 | Avg:-0.05 | Min:-0.17 | Max: 0.11\n",
      "    Cluster 4: Size:410 | Avg:-0.06 | Min:-0.16 | Max: 0.10\n"
     ]
    }
   ],
   "source": [
    "clustering, cluster_labels = mbkmeans_clusters(\n",
    "\tX=vectorized_docs,\n",
    "    k=n_clusters,\n",
    "    mb=500,\n",
    "    print_silhouette_values=True,\n",
    ")\n",
    "df_clusters = pd.DataFrame({\n",
    "    \"text\": df[\"body\"].values,\n",
    "    \"tokens\": [\" \".join(text) for text in processed_corpus],\n",
    "    \"cluster\": cluster_labels\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Top terms* por cluster (basado en los centroides de los clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most representative terms per cluster (based on centroids):\n",
      "Cluster 0: macristo macri~~. macris macrium macri-pe√±o macri mracri macrismo macristaaaa -macri \n",
      "Cluster 1: desgraciada desgracia felizmente \\*gracia gracia desgraciado agradecida graciassss graciasss desagraciado \n",
      "Cluster 2: perpet√∫ar perfumar perm√≠tanmar perimetro perpetuar permanente perturbadoras percatar perpetrado perturbador \n",
      "Cluster 3: viejo abejo cejo vieja.me.dejo viejo= pendeviejo viviente rejo festejo vieja \n",
      "Cluster 4: rindo festilindo lindo gondo brindo cundo hundo oriundo swgundo trotamundo \n",
      "Cluster 5: argentina?.\"al argentina-per√∫ argentiniar argentino argentinatm argentina1234 argentinans aegentino argentinoo argentinadepie \n",
      "Cluster 6: cavar caduc√≥ cat6 ca√∫s caliente cararrotar carvajal caroyar capua ca√≠do \n",
      "Cluster 7: desmadrar enterrar desenterrar yikar oar depurar tetrar descifrar delirar r√≠ar \n",
      "Cluster 8: agua calientenla caliente agua\\ piojo aguja aguila freelo riachuelo pintamelo \n",
      "Cluster 9: pesos d√≥lar peso dolar d√≥lar_har ~~dolares~~ embolar bipolar am√°ndolar indic√°ndolar \n",
      "Cluster 10: hablar hablarl cablar hablarles hablarme hablaba ensamblar habl√°bar blar habla \n",
      "Cluster 11: vo -vo vox vofi vodka vodi voraz vos octavo vot√°s \n",
      "Cluster 12: ineficiente deficiente injusticiar eficiente coeficiente intervencionista creciente combatiente derrotar intervencionismo \n",
      "Cluster 13: theory thought theyll th thank though without thanks third thots \n",
      "Cluster 14: mano el.mano mano(si rumano manos manor piromano manoplar lavamano romano \n",
      "Cluster 15: matar atar mametar jonatar maorretar johnatar anismanlomatar rematar f√≠jatar marmotar \n",
      "Cluster 16: laempresanoesresponsablepordesgraciaspersonalnisupersonalestargenuinamenteinteresadoenloqueocurraenlavidadelcliente administrativo ineficiente descentralizaci√≥n administrar regularmente industrializaci√≥n reglamenteci√≥n conscriptar concientizar \n",
      "Cluster 17: faltar falt falta faltan faltariar faltant fal gibraltar asfaltar false \n",
      "Cluster 18: ‚£∞ ‚†≥ ‚°¶ ‚°∂ 00:02:20 ‚†ü ‚¢ó ‚†û ‚¢ò ‚†∂ \n",
      "Cluster 19: hombre homre->mujer ~~hombre hombr mujer exmujer ~~nombre~~ mujeres nombre hombro \n",
      "Cluster 20: laempresanoesresponsablepordesgraciaspersonalnisupersonalestargenuinamenteinteresadoenloqueocurraenlavidadelcliente intraducible descentralizado inexistente intransigente ralentizado detergente autoadministrado ineficiente creciente \n",
      "Cluster 21: t√©n obt√©n t√© c√©n m√°g√©n sart√©n av√©n tengan mant√©n am√©n \n",
      "Cluster 22: terrateniente coeficiente terriblemente inocente corruptamente fehaciente ineficiente intermitente inconsciente consciente \n",
      "Cluster 23: dar odar rodar gaydar sudar dartagnar amoldar apodar leudar darl \n",
      "Cluster 24: jajaja jajajaja jajajaajaja jajajar jajajajar jajajaa jajajejox jajajaajj jajajaaj jajajajajar \n",
      "Cluster 25: ver aver ver√≠ar vancouver verl abrosver elver ver√© over sever \n",
      "Cluster 26: poner iponer ponerl oponer ponerlo disponer exponer reponer oner sooner \n",
      "Cluster 27: mes mesas mes√≠as mes√≠a meshi james times mesopotamica mesi mesa \n",
      "Cluster 28: yo hoyo qcyo joyo √©l yayo yokozuna anyo oclayo hoja \n",
      "Cluster 29: comeriia com√≠ coml comit√© comecar comerme comib com√©s comercios comi \n",
      "Cluster 30: mirar \\*mirar admirar lirar elvirar girar juirar dirar s√°tirar quirar \n",
      "Cluster 31: l√©xico eico ex√≥tico c√≠vico p√∫bico g√≥tico l√∫dico soico ac√∫stico ps√≠quico \n",
      "Cluster 32: esperabar desesperar esperar 2)esperar espesar se√±al_esperar inesperado esperanzado assperar esperanzador \n",
      "Cluster 33: venir devenir veniar venirte cezinkoenir venie veni cenir venis venitir \n",
      "Cluster 34: idea ideapad ideal idem ide idearia idk dea idealizar idealizaci√≥n \n",
      "Cluster 35: \\-car fiacar orcar bacar yikar oar acar azkar cacar car \n",
      "Cluster 36: pagarar pagar pagartar cobraplar pagartelo \\*pagar pagarl pagarlo cobra prepagar \n",
      "Cluster 37: auto arauto ruto luto flauto yuto autoretrato incauto analauto moto \n",
      "Cluster 38: salir sal√∫dalir sali saliva salio salimos saliar pasalir salia sal√≠a \n",
      "Cluster 39: sube subite autom√°ticamente)._^(feedback montevideo subterr√°neo elclubdelosprogramadoreseneuropa encarecidamente p√°g√≠n fotogr√°ficamente publicaci√≥n \n",
      "Cluster 40: tomar \\*tomar tomate tomarlo tomatel√° tomatela retomar tomar√≠a tomarno tomatelas \n",
      "Cluster 41: escuchar escucharar escucharlo escuchaba edcuchar escucha -cuchar escuchado escuch√°s escuchastar \n",
      "Cluster 42: hijo hijaeputa hijos hijodeputo hij hijota putamadre hijo_puta hijaput laputamadre \n",
      "Cluster 43: gustar degustar gustari frustar gustarir gustaba gustaria gustavo gusta asustar \n",
      "Cluster 44: empecinar empezar bezar determinar terminar exterminar empeorar germinar fulminar terminal \n",
      "Cluster 45: va va- va\\ vaa val√≠a vas valdeszeva pod vaya cueva \n",
      "Cluster 46: laempresanoesresponsablepordesgraciaspersonalnisupersonalestargenuinamenteinteresadoenloqueocurraenlavidadelcliente conscientemente eficientemente ineficiente coeficiente deficiente eficiente creciente exigente subconsciente \n",
      "Cluster 47: ir iriir iir yyyir oir j√≥vir irir qcir cdoir tujir \n",
      "Cluster 48: quedar quedabar quedariar quedara quedamo queda quedado quedartir quedo quedate \n",
      "Cluster 49: re reabre rex re√≠ repostre refiere rel rev reev reltih \n",
      "Cluster 50: thought though wouldn¬¥t wouldnt its thank without theory we theyll \n",
      "Cluster 51: paso pasar pas√© pasarl pas√° pasamo pase pas√°s pases pasar√°s \n",
      "Cluster 52: milei-espert milei milei.y mileinial milei‚Äôs \\-[*espert espert espertma spert kazuspert \n",
      "Cluster 53: tener tenerla obtener \\-tener abstener tenembaum tenete detener retener tenenbaum \n",
      "Cluster 54: ado n√≥mado vado naado rayado evado wado ahumado hado zanjado \n",
      "Cluster 55: extender entender encender desentender tender pretender bender ofender atender revender \n",
      "Cluster 56: votar votarar votarl voto votos votarlo votastar sigan_votar votaba votanso \n",
      "Cluster 57: trabanar var√≠ar ganancio estanciero equilibrar financieramente equivaliar ganancias irregulares irregular \n",
      "Cluster 58: decir \\-decir decirno decibl deci decidansir decimir decidi decirtar decirmelo \n",
      "Cluster 59: ac√° ac√°bo ac acv acn√© \\*sac√° aca acaa acab√© acab√°s \n",
      "Cluster 60: ley leer le√≠ le√© ley_etiquetar leer√© ley_etiquetado leyends leen leerlo \n",
      "Cluster 61: anti-peronismo peronmacrismo anti-zurdismo peronismo absurd√≠smo stalinismo abismo imperialismo estalinismo ultraderecha-neoliberal-imperialista-cipaya-procia \n",
      "Cluster 62: tipo tipoel tips tipea zalipo logotipo prototipo pipo tipeo tip \n",
      "Cluster 63: pa√≠s expa√≠s p√≠s pb√≠s m√≠s v√≠s insurgente exigente inmigrantes+planeros+clientelismo pobrismo \n",
      "Cluster 64: precio precios_cuidado control_precio precipicio crecio preciosbardeado aprecio precie precisar precio-calidad \n",
      "Cluster 65: √©l cosechar obligadamente inocente obligatoriamente corruptamente intermitente derrochar intento correctamente \n",
      "Cluster 66: a√±o a√±s a√±o.~~ a√±os ermita√±o equilibrado secuestrado creciente electorado secuestrador \n",
      "Cluster 67: vo vox -vo vofi vodka vodi vos voraz voz llvo \n",
      "Cluster 68: funci√≥n_gobierno gobiedno gobierno montogobierno funci√≥n_gobiernir cobierno gobiernola anses-gobierno.com gobierne goberno \n",
      "Cluster 69: ah ^ah -ah tdah ahy ahii ~~ah \\-ah ahh macristaaaa \n"
     ]
    }
   ],
   "source": [
    "print(\"Most representative terms per cluster (based on centroids):\")\n",
    "for i in range(n_clusters):\n",
    "    tokens_per_cluster = \"\"\n",
    "    most_representative = model.wv.most_similar(positive=[clustering.cluster_centers_[i]], topn=10)\n",
    "    for t in most_representative:\n",
    "        tokens_per_cluster += f\"{t[0]} \"\n",
    "    print(f\"Cluster {i}: {tokens_per_cluster}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Top terms* por cluster (basado en las palabras m√°s frecuentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: macri(76) alberto(54) /s(40) cristina(28) kirchner(18) \n",
      "Cluster 1: vida(167) gracias(139) op(33) gracia(32) dios(23) \n",
      "Cluster 2: perro(91) llamar(82) andar(27) pensar(23) mandar(23) \n",
      "Cluster 3: viejo(226) hijo(58) vivir(39) morir(34) puta(33) \n",
      "Cluster 4: lindo(110) mundo(84) che(32) mierda(31) mierdo(25) \n",
      "Cluster 5: argentino(230) argentina(153) mundo(16) r(15) pasar(13) \n",
      "Cluster 6: cara(107) casa(101) caer(84) cagar(63) calle(57) \n",
      "Cluster 7: dejar(212) pasar(60) entrar(57) tirar(49) querer(30) \n",
      "Cluster 8: agua(88) calor(81) orto(45) ojo(41) aire(39) \n",
      "Cluster 9: peso(117) dolar(110) pesos(72) d√≥lar(64) d√≥lares(33) \n",
      "Cluster 10: hablar(181) dejar(8) cabeza(7) escuchar(7) cristina(6) \n",
      "Cluster 11: vo(365) decir(30) √©l(28) so(16) pasar(15) \n",
      "Cluster 12: gente(82) vivir(75) provincia(56) nacional(44) querer(41) \n",
      "Cluster 13: the(195) of(74) and(70) you(63) to(60) \n",
      "Cluster 14: mano(100) man(6) querer(5) humano(4) duro(4) \n",
      "Cluster 15: matar(84) robar(33) contar(15) nisman(10) evitar(9) \n",
      "Cluster 16: dato(53) p√∫blico(45) √©l(43) social(41) pasar(41) \n",
      "Cluster 17: faltar(67) falta(47) favor(35) se√±or(19) respeto(10) \n",
      "Cluster 18: x200b(58) Ô∏è(50) >(35) don(31) san(25) \n",
      "Cluster 19: hombre(91) mujer(87) nombre(30) gris(7) serio(6) \n",
      "Cluster 20: decir(60) hacer(58) pensar(53) pedir(50) pasar(46) \n",
      "Cluster 21: t√©n(151) pod(9) arte(7) √©l(7) razon(7) \n",
      "Cluster 22: √©l(470) querer(153) amigo(110) hacer(71) seguro(48) \n",
      "Cluster 23: dar(201) √©l(67) vuelta(14) querer(13) gente(8) \n",
      "Cluster 24: jaja(87) jajaja(72) jajajar(54) jajajaja(46) jajaj(36) \n",
      "Cluster 25: ver(440) hacer(31) √©l(15) parecer(14) foto(14) \n",
      "Cluster 26: poner(188) √©l(15) pasar(6) az√∫car(5) familia(4) \n",
      "Cluster 27: mes(99) a√±o(6) √©l(6) m√≠nimo(4) pagar(4) \n",
      "Cluster 28: yo(242) decir(21) ir(18) √©l(17) decimir(13) \n",
      "Cluster 29: comer(146) comida(53) comprar(36) come(35) carne(30) \n",
      "Cluster 30: mirar(76) tirar(35) raro(9) ojo(7) pasar(5) \n",
      "Cluster 31: √∫nico(69) rico(53) chico(30) pol√≠tico(21) asco(16) \n",
      "Cluster 32: esperar(117) estudiar(34) clase(12) carrera(9) estudio(7) \n",
      "Cluster 33: venir(246) a√±o(16) semana(13) mes(10) pasar(7) \n",
      "Cluster 34: idea(95) persona(6) pasar(5) jaja(4) liberal(4) \n",
      "Cluster 35: sacar(70) car(61) buscar(39) par(14) √©l(10) \n",
      "Cluster 36: pagar(222) impuesto(104) plata(59) plan(55) cobrar(49) \n",
      "Cluster 37: auto(117) foto(20) moto(18) punto(11) pasar(11) \n",
      "Cluster 38: salir(204) sal(14) correr(10) calle(9) salio(7) \n",
      "Cluster 39: foto(132) video(77) dato(42) alto(41) sub(40) \n",
      "Cluster 40: tomar(177) mate(66) malo(18) dulce(14) gente(12) \n",
      "Cluster 41: escuchar(72) m√∫sica(17) conocer(10) voz(9) garchar(8) \n",
      "Cluster 42: hijo(65) puta(45) puto(12) madre(7) remil(5) \n",
      "Cluster 43: gustar(146) gusto(27) gusta(21) gente(6) fernet(6) \n",
      "Cluster 44: empezar(69) terminar(38) jugar(9) querer(8) serie(6) \n",
      "Cluster 45: va(203) pod(21) √©l(11) vo(9) t√©n(9) \n",
      "Cluster 46: gente(227) persona(129) tema(101) problema(72) pensar(66) \n",
      "Cluster 47: ir(73) √©l(8) decir(6) pasar(6) querer(5) \n",
      "Cluster 48: quedar(104) quedo(7) pensar(7) gente(5) querer(5) \n",
      "Cluster 49: re(163) puta_madre(8) loco(8) ver(8) ah(7) \n",
      "Cluster 50: ‚†Ä(685) the(69) to(56) and(36) of(32) \n",
      "Cluster 51: paso(117) pasar(88) paz(10) pase(9) pan(8) \n",
      "Cluster 52: milei(180) espert(121) ca√±o(43) debate(33) √©l(19) \n",
      "Cluster 53: tener(217) ten√©s(54) a√±o(15) tenia(14) √©l(11) \n",
      "Cluster 54: lado(35) pelado(15) pedo(12) grado(10) armado(9) \n",
      "Cluster 55: entender(106) vender(99) aprender(52) depender(29) gente(20) \n",
      "Cluster 56: votar(127) voto(115) perder(40) peronismo(29) ganar(24) \n",
      "Cluster 57: ganar(84) laburo(73) semana(67) mes(61) pasar(56) \n",
      "Cluster 58: decir(193) favor(7) pensar(6) decis(5) dej(5) \n",
      "Cluster 59: ac√°(142) pobre(13) aca(13) argentina(7) sub(7) \n",
      "Cluster 60: leer(149) ley(148) post(44) comentario(34) sub(27) \n",
      "Cluster 61: peronismo(104) peronista(97) k(59) votar(52) milei(51) \n",
      "Cluster 62: tipo(281) tiro(39) tirar(22) pobre(21) capaz(15) \n",
      "Cluster 63: pa√≠s(317) argentina(18) gente(17) mundo(16) pasar(16) \n",
      "Cluster 64: precio(219) controlar(21) comprar(19) control(19) producto(18) \n",
      "Cluster 65: √©l(956) hacer(59) ver(45) querer(39) gente(38) \n",
      "Cluster 66: a√±o(381) pasar(20) venir(19) pensar(14) esperar(13) \n",
      "Cluster 67: vo(55) favor(3) decir(3) gracias(3) favor_robo(2) \n",
      "Cluster 68: gobierno(195) nacional(32) terrorista(23) elecci√≥n(18) inflaci√≥n(13) \n",
      "Cluster 69: ah(127) macri(14) pacer(6) cu(6) decir(5) \n"
     ]
    }
   ],
   "source": [
    "for i in range(n_clusters):\n",
    "    tokens_per_cluster = \"\"\n",
    "    most_frequent = Counter(\" \".join(df_clusters.query(f\"cluster == {i}\")[\"tokens\"]).split()).most_common(5)\n",
    "    for t in most_frequent:\n",
    "        tokens_per_cluster += f\"{t[0]}({str(t[1])}) \"\n",
    "    print(f\"Cluster {i}: {tokens_per_cluster}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recupere los documentos m√°s representativos (basados en los centroides de los cl√∫steres) para un cluster en particular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No peronistas: Yrigoyen (1916 - 1922), Alvear y Macri.. Peronistas: Per√≥n (1946 - 1952), Menem, Kirchner y CFK.. &#x200B;. Antes del 2000, la diferencia no era tan grande.\n",
      "-------------\n",
      "Macri=bad Cristina=good los pasacalles de mayra dicen vota la lista de Cristina y al bigote que tenemos de presidente lo limpiaron de todos los afiches.\n",
      "-------------\n",
      "Es una catarata infinita de falacias y sesgos, podr√≠a escribir un libro comentado lo objetable. Sigue siendo mejor que un talib√°n kirchnerista pero .. me hace acordar mucho a macri, kirchnerismo con buenos modales\n",
      "-------------\n",
      "La oposici√≥n conformada por Fernanda Vallejos, El Gato Silvestre, y liderado por la mism√≠sima Cristina F Kirchner\n",
      "-------------\n",
      "No entiendo qu√© piensan cuando reciben noticias como lo de la plata enterrada de M√°ximo, los d√≥lares tra√≠dos de contrabando para campa√±a de Antonini Wilson, la rosadita y todas esas cosas ya probadas ¬øSolo ah, pero Macri?\n",
      "-------------\n",
      "Qu√© es lo q festejan? Por qu√© sonr√≠en, cantan y alaban a los pol√≠ticos? Acaso no les cabe la posibilidad de que Cristina y N√©stor sean responsables de estado del pa√≠s, tambi√©n? Todo lo malo es solo por Macri?\n",
      "-------------\n",
      "Falto Lazaro Baez/Nestor Kirchner en esa infografia.\n",
      "-------------\n",
      "[[C1-FECHA 4] Mc Cree como Acuario (?](http://imgur.com/gallery/8mBx4W8). he descubierto que con Macri somos del mismo signo (?. espero que les guste :3. ~~u/unLucas agarrate catalina porque hoy me llevo el 1¬∞ puesto (?~~\n",
      "-------------\n",
      "Ahhh mi buen representante sindical.... CALO TE BAJASTE LOS LOMPA CON LA CHORRA DE CRISTINA 2 A√ëOS SEGUIDOS.. Disculpen el exabrupto, esta persona habla de adoctrinar. Por eso el pais esta como esta, por que adoctrinaron al 35%.\n",
      "-------------\n",
      "En formorsa creo. Estan las estatuas de albert, maradona y los peronchos.\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "test_cluster = 0\n",
    "most_representative_docs = np.argsort(\n",
    "    np.linalg.norm(vectorized_docs - clustering.cluster_centers_[test_cluster], axis=1)\n",
    ")\n",
    "for d in most_representative_docs[:10]:\n",
    "    print( df[\"body\"].values[d])\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print(len(vectorized_docs))\n",
    "#print(vectorized_docs[0])\n",
    "\n",
    "test_v = vectorize([['defender', 'peso', 'siente', 'coraz√≥n', 'compro', 'pesos', 'tasa', 'fijo', 'a√±o']], model=model)\n",
    "prediction = clustering.predict(test_v)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = pd.read_csv(TEXT_FILE_READ)\n",
    "\n",
    "def get_cluster(row):\n",
    "    test_v = vectorize([row], model=model)\n",
    "    return clustering.predict(test_v)\n",
    "\n",
    "reddit['cluster'] = reddit.apply(lambda row: get_cluster(row['lemma_tokens']) , axis = 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>flair</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>body</th>\n",
       "      <th>comment_parent_id</th>\n",
       "      <th>is_replay</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>Unnamed: 12</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "      <th>lemma_tokens</th>\n",
       "      <th>body_preprocessing</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>hfw14mt</td>\n",
       "      <td>Discusionüßê</td>\n",
       "      <td>1</td>\n",
       "      <td>todo para decir que tapaste el ba√±o. tira un b...</td>\n",
       "      <td>q44kw3</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['tapastir', 'ba√±o', 'tirar', 'balde', 'aguo']</td>\n",
       "      <td>tapastir ba√±o tirar balde aguo</td>\n",
       "      <td>[18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>hfw41eh</td>\n",
       "      <td>Discusionüßê</td>\n",
       "      <td>0</td>\n",
       "      <td>sopapa primero master, si hay tap√≥n te vas a t...</td>\n",
       "      <td>hfw14mt</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['sopapa', 'master', 'tap√≥n', 'va', 'te√±ir', '...</td>\n",
       "      <td>sopapa master tap√≥n va te√±ir medio</td>\n",
       "      <td>[18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>hfw1ao2</td>\n",
       "      <td>Discusionüßê</td>\n",
       "      <td>0</td>\n",
       "      <td>Usas la sopapa, o tiras agua caliente con un b...</td>\n",
       "      <td>q44kw3</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['sopapo', 'tira', 'agua', 'caliente', 'balde']</td>\n",
       "      <td>sopapo tira agua caliente balde</td>\n",
       "      <td>[18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>hfw3jof</td>\n",
       "      <td>Discusionüßê</td>\n",
       "      <td>2</td>\n",
       "      <td>Lo que he probado que siempre me dio resultado...</td>\n",
       "      <td>q44kw3</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['probado', 'resultado', 'sellar', 'boca', 'in...</td>\n",
       "      <td>probado resultado sellar boca inodoro tirar ca...</td>\n",
       "      <td>[18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>hfw6v4i</td>\n",
       "      <td>Discusionüßê</td>\n",
       "      <td>0</td>\n",
       "      <td>Estas cobrando por dar mantenimiento y no sabe...</td>\n",
       "      <td>q44kw3</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['cobrar', 'mantenimiento', 'carajo', 'kjjjjjj...</td>\n",
       "      <td>cobrar mantenimiento carajo kjjjjjjjjj vivirio...</td>\n",
       "      <td>[18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>hfw26iv</td>\n",
       "      <td>Discusionüßê</td>\n",
       "      <td>0</td>\n",
       "      <td>Si tenes algo con punta, metelo y hace un poco...</td>\n",
       "      <td>q44kw3</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['t√©n', 'punto', 'metelo', 'fuerza', 'romper',...</td>\n",
       "      <td>t√©n punto metelo fuerza romper tapo ba√±o tirar...</td>\n",
       "      <td>[18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>hfw2gof</td>\n",
       "      <td>Discusionüßê</td>\n",
       "      <td>1</td>\n",
       "      <td>Con una manguera para regar el jard√≠n, si tene...</td>\n",
       "      <td>q44kw3</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['regar', 'jard√≠n', 't√©n', 'pod', 'probar']</td>\n",
       "      <td>regar jard√≠n t√©n pod probar</td>\n",
       "      <td>[18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>hfw5s13</td>\n",
       "      <td>Discusionüßê</td>\n",
       "      <td>0</td>\n",
       "      <td>despues regas el jardin y se lava sola, solo q...</td>\n",
       "      <td>hfw2gof</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['rega', 'jardin', 'lava', 'ten√©s', 'lavarte',...</td>\n",
       "      <td>rega jardin lava ten√©s lavarte mano pulgar chorro</td>\n",
       "      <td>[18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>hfw3air</td>\n",
       "      <td>Discusionüßê</td>\n",
       "      <td>0</td>\n",
       "      <td>La respuesta real es que se venden unos ca√±os ...</td>\n",
       "      <td>q44kw3</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['respuesta', 'real', 'vender', 'ca√±o', 'alamb...</td>\n",
       "      <td>respuesta real vender ca√±o alambrado decir ca√±...</td>\n",
       "      <td>[18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>hfvxa6w</td>\n",
       "      <td>Discusionüßê</td>\n",
       "      <td>3</td>\n",
       "      <td>Mi alfajor favorito es el Havana</td>\n",
       "      <td>q443eo</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['alfajor', 'favorito', 'hav√°n']</td>\n",
       "      <td>alfajor favorito hav√°n</td>\n",
       "      <td>[18]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score       id       flair  comms_num  \\\n",
       "0      1  hfw14mt  Discusionüßê          1   \n",
       "1      1  hfw41eh  Discusionüßê          0   \n",
       "2      1  hfw1ao2  Discusionüßê          0   \n",
       "3      1  hfw3jof  Discusionüßê          2   \n",
       "4      1  hfw6v4i  Discusionüßê          0   \n",
       "5      1  hfw26iv  Discusionüßê          0   \n",
       "6      1  hfw2gof  Discusionüßê          1   \n",
       "7      1  hfw5s13  Discusionüßê          0   \n",
       "8      1  hfw3air  Discusionüßê          0   \n",
       "9      7  hfvxa6w  Discusionüßê          3   \n",
       "\n",
       "                                                body comment_parent_id  \\\n",
       "0  todo para decir que tapaste el ba√±o. tira un b...            q44kw3   \n",
       "1  sopapa primero master, si hay tap√≥n te vas a t...           hfw14mt   \n",
       "2  Usas la sopapa, o tiras agua caliente con un b...            q44kw3   \n",
       "3  Lo que he probado que siempre me dio resultado...            q44kw3   \n",
       "4  Estas cobrando por dar mantenimiento y no sabe...            q44kw3   \n",
       "5  Si tenes algo con punta, metelo y hace un poco...            q44kw3   \n",
       "6  Con una manguera para regar el jard√≠n, si tene...            q44kw3   \n",
       "7  despues regas el jardin y se lava sola, solo q...           hfw2gof   \n",
       "8  La respuesta real es que se venden unos ca√±os ...            q44kw3   \n",
       "9                   Mi alfajor favorito es el Havana            q443eo   \n",
       "\n",
       "  is_replay Unnamed: 7 Unnamed: 8 Unnamed: 9 Unnamed: 10 Unnamed: 11  \\\n",
       "0     False        NaN        NaN        NaN         NaN         NaN   \n",
       "1      True        NaN        NaN        NaN         NaN         NaN   \n",
       "2     False        NaN        NaN        NaN         NaN         NaN   \n",
       "3     False        NaN        NaN        NaN         NaN         NaN   \n",
       "4     False        NaN        NaN        NaN         NaN         NaN   \n",
       "5     False        NaN        NaN        NaN         NaN         NaN   \n",
       "6     False        NaN        NaN        NaN         NaN         NaN   \n",
       "7      True        NaN        NaN        NaN         NaN         NaN   \n",
       "8     False        NaN        NaN        NaN         NaN         NaN   \n",
       "9     False        NaN        NaN        NaN         NaN         NaN   \n",
       "\n",
       "  Unnamed: 12 Unnamed: 13 Unnamed: 14  \\\n",
       "0         NaN         NaN         NaN   \n",
       "1         NaN         NaN         NaN   \n",
       "2         NaN         NaN         NaN   \n",
       "3         NaN         NaN         NaN   \n",
       "4         NaN         NaN         NaN   \n",
       "5         NaN         NaN         NaN   \n",
       "6         NaN         NaN         NaN   \n",
       "7         NaN         NaN         NaN   \n",
       "8         NaN         NaN         NaN   \n",
       "9         NaN         NaN         NaN   \n",
       "\n",
       "                                        lemma_tokens  \\\n",
       "0     ['tapastir', 'ba√±o', 'tirar', 'balde', 'aguo']   \n",
       "1  ['sopapa', 'master', 'tap√≥n', 'va', 'te√±ir', '...   \n",
       "2    ['sopapo', 'tira', 'agua', 'caliente', 'balde']   \n",
       "3  ['probado', 'resultado', 'sellar', 'boca', 'in...   \n",
       "4  ['cobrar', 'mantenimiento', 'carajo', 'kjjjjjj...   \n",
       "5  ['t√©n', 'punto', 'metelo', 'fuerza', 'romper',...   \n",
       "6        ['regar', 'jard√≠n', 't√©n', 'pod', 'probar']   \n",
       "7  ['rega', 'jardin', 'lava', 'ten√©s', 'lavarte',...   \n",
       "8  ['respuesta', 'real', 'vender', 'ca√±o', 'alamb...   \n",
       "9                   ['alfajor', 'favorito', 'hav√°n']   \n",
       "\n",
       "                                  body_preprocessing cluster  \n",
       "0                     tapastir ba√±o tirar balde aguo    [18]  \n",
       "1                 sopapa master tap√≥n va te√±ir medio    [18]  \n",
       "2                    sopapo tira agua caliente balde    [18]  \n",
       "3  probado resultado sellar boca inodoro tirar ca...    [18]  \n",
       "4  cobrar mantenimiento carajo kjjjjjjjjj vivirio...    [18]  \n",
       "5  t√©n punto metelo fuerza romper tapo ba√±o tirar...    [18]  \n",
       "6                        regar jard√≠n t√©n pod probar    [18]  \n",
       "7  rega jardin lava ten√©s lavarte mano pulgar chorro    [18]  \n",
       "8  respuesta real vender ca√±o alambrado decir ca√±...    [18]  \n",
       "9                             alfajor favorito hav√°n    [18]  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show\n",
    "reddit.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit.to_csv(TEXT_SAVE_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'docs/testfasttext/0.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-1823d2318032>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mreddit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreddit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cluster\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'flair'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'docs/testfasttext/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3385\u001b[0m         )\n\u001b[1;32m   3386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3387\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3388\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3389\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1081\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         )\n\u001b[0;32m-> 1083\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \"\"\"\n\u001b[1;32m    227\u001b[0m         \u001b[0;31m# apply compression and byte/text conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         with get_handle(\n\u001b[0m\u001b[1;32m    229\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'docs/testfasttext/0.csv'"
     ]
    }
   ],
   "source": [
    "for i in range(n_clusters):\n",
    "    reddit[(reddit[\"cluster\"] == i)][['flair', 'body']].to_csv('docs/testfasttext/' + str(i) + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
