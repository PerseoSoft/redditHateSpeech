{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Neuronales\n",
    "\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importanci√≥n de librer√≠a requeridas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definici√≥n de variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_FILE_READ = 'docs/preprocessing_reddit_data.csv'\n",
    "TEXT_SAVE_FILE = 'docs/reddit_data_lda.csv'\n",
    "FILENAME_PICKLE = \"docs/tmpreddit.pickle\"\n",
    "n_clusters = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de los comentarios de Reddit\n",
    "\n",
    "Los comentarios fueron previamente preprocesados (Ver en TODO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(FILENAME_PICKLE, 'rb') as f:\n",
    "    df = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(df['lemma_tokens'])\n",
    "\n",
    "# Filtering Extremes\n",
    "id2word.filter_extremes(no_below=2, no_above=.99)\n",
    "\n",
    "# Creating a corpus object\n",
    "corpus = [id2word.doc2bow(d) for d in df['lemma_tokens']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_corpus = df['lemma_tokens']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=processed_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.train(processed_corpus, total_examples=len(processed_corpus), epochs=100)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = []\n",
    "vocabulary = list(model.wv.key_to_index)\n",
    "\n",
    "for key in model.wv.key_to_index:\n",
    "    word_vecs.append(model.wv[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jajajajjajaajaj', 0.9570168256759644),\n",
       " ('bottle', 0.9228257536888123),\n",
       " ('laconcho', 0.9012131094932556),\n",
       " ('branding', 0.8885334134101868),\n",
       " ('submarinar', 0.8106595873832703),\n",
       " ('meanies', 0.7711156010627747),\n",
       " ('golden', 0.7673962116241455),\n",
       " ('frondizi', 0.6835584044456482),\n",
       " ('gustavo', 0.676913321018219),\n",
       " ('oct', 0.6762269139289856)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# algunas predicciones\n",
    "\n",
    "model.wv.most_similar(\"rucula\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos los cl√∫sters\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "X_wvkm = kmeans.fit_transform(word_vecs)\n",
    "y_wvkm = kmeans.predict(word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in range(n_clusters):\n",
    "    mask = X_wvkm[y_wvkm == cluster]\n",
    "    idx_sort = np.argsort(X_wvkm[:,cluster])\n",
    "    words = [vocabulary[x] for x in idx_sort[:20]]\n",
    "\n",
    "    print(\"Cl√∫ster %d:\" % cluster, end='')\n",
    "    print()\n",
    "    for w in words:\n",
    "        print(' %s' % w, end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0mTraceback (most recent call last)",
      "\u001B[0;32m<ipython-input-21-8e64c508c0ab>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mX\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"Nothing is easy in cricket. Maybe when you watch it on TV, it looks easy. But it is not. You have to use your brain and time the ball.\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mpredicted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpredicted\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Word2Vec' object has no attribute 'transform'"
     ]
    }
   ],
   "source": [
    "X = model.transform([\"Nothing is easy in cricket. Maybe when you watch it on TV, it looks easy. But it is not. You have to use your brain and time the ball.\"])\n",
    "predicted = model.predict(X)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'peron k'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m\u001B[0m",
      "\u001B[0;31mValueError\u001B[0mTraceback (most recent call last)",
      "\u001B[0;32m<ipython-input-14-adecac7fad74>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m#reddit = pd.read_csv(TEXT_FILE_READ)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mtest\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkmeans\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'peron k'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py\u001B[0m in \u001B[0;36mpredict\u001B[0;34m(self, X, sample_weight)\u001B[0m\n\u001B[1;32m   1154\u001B[0m         \u001B[0mcheck_is_fitted\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1155\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1156\u001B[0;31m         \u001B[0mX\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_check_test_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1157\u001B[0m         \u001B[0mx_squared_norms\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrow_norms\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msquared\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1158\u001B[0m         \u001B[0msample_weight\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_check_sample_weight\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msample_weight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py\u001B[0m in \u001B[0;36m_check_test_data\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    856\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    857\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_check_test_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 858\u001B[0;31m         X = self._validate_data(X, accept_sparse='csr', reset=False,\n\u001B[0m\u001B[1;32m    859\u001B[0m                                 \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat64\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    860\u001B[0m                                 order='C', accept_large_sparse=False)\n",
      "\u001B[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/base.py\u001B[0m in \u001B[0;36m_validate_data\u001B[0;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[1;32m    419\u001B[0m             \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    420\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0my\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'no_validation'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 421\u001B[0;31m             \u001B[0mX\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcheck_array\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mcheck_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    422\u001B[0m             \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    423\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36minner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     61\u001B[0m             \u001B[0mextra_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mall_args\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mextra_args\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m             \u001B[0;31m# extra_args > 0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36mcheck_array\u001B[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001B[0m\n\u001B[1;32m    614\u001B[0m                     \u001B[0marray\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0marray\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcasting\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"unsafe\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    615\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 616\u001B[0;31m                     \u001B[0marray\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0morder\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0morder\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    617\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mComplexWarning\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mcomplex_warning\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    618\u001B[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001B[0;32m/opt/conda/lib/python3.8/site-packages/numpy/core/_asarray.py\u001B[0m in \u001B[0;36masarray\u001B[0;34m(a, dtype, order, like)\u001B[0m\n\u001B[1;32m    100\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0m_asarray_with_like\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0morder\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0morder\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlike\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlike\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 102\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0morder\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0morder\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    103\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    104\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: could not convert string to float: 'peron k'"
     ]
    }
   ],
   "source": [
    "#reddit = pd.read_csv(TEXT_FILE_READ)\n",
    "\n",
    "test = kmeans.predict(['peron k'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m\u001B[0m",
      "\u001B[0;31mNameError\u001B[0mTraceback (most recent call last)",
      "\u001B[0;32m<ipython-input-15-3bc6909a44d2>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     27\u001B[0m     \u001B[0;32mreturn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msent_topics_df\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 29\u001B[0;31m \u001B[0mdf_topic_sents_keywords\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mformat_topics_sentences\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mldamodel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbase_model\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcorpus\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcorpus\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtexts\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mreddit\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'base_model' is not defined"
     ]
    }
   ],
   "source": [
    "reddit = pd.read_csv(TEXT_FILE_READ)\n",
    "\n",
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n",
    "        # row = sorted(row, key=lambda x: (x[1]), reverse=True) # old line\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0: # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "                #ent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4)]), ignore_index=True)\n",
    "                #print(sent_topics_df)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "    #sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    #contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, texts], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=base_model, corpus=corpus, texts=reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Perc_Contribution</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>flair</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>body</th>\n",
       "      <th>comment_parent_id</th>\n",
       "      <th>is_replay</th>\n",
       "      <th>lemma_tokens</th>\n",
       "      <th>body_preprocessing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.2531</td>\n",
       "      <td>√©l, recordar, pegar, √∫nico, robar, barrio, pen...</td>\n",
       "      <td>1</td>\n",
       "      <td>hgw77qe</td>\n",
       "      <td>Pol√≠ticaüèõÔ∏è</td>\n",
       "      <td>0</td>\n",
       "      <td>Iba a decir, bue si lo saco de su bolsillo... ...</td>\n",
       "      <td>q9imco</td>\n",
       "      <td>False</td>\n",
       "      <td>['bue', 'saco', 'bolsillo', 'recorder', 'hdp',...</td>\n",
       "      <td>bue saco bolsillo recorder hdp mantener alcanz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2598</td>\n",
       "      <td>perro, nik, meme, gobierno, explicar, it, teni...</td>\n",
       "      <td>1</td>\n",
       "      <td>hgw7dci</td>\n",
       "      <td>Pol√≠ticaüèõÔ∏è</td>\n",
       "      <td>0</td>\n",
       "      <td>Se volvio un meme el bot del dolar?</td>\n",
       "      <td>hgw666m</td>\n",
       "      <td>True</td>\n",
       "      <td>['volvio', 'meme', 'dolar']</td>\n",
       "      <td>volvio meme dolar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.3279</td>\n",
       "      <td>falacia, decir, gratis, k, joda, pa√≠s, mandar,...</td>\n",
       "      <td>1</td>\n",
       "      <td>hgw69er</td>\n",
       "      <td>Humor:snoo_joy:</td>\n",
       "      <td>0</td>\n",
       "      <td>Este Esteban Lamothe estaba en la ficci√≥n de u...</td>\n",
       "      <td>q9i4uj</td>\n",
       "      <td>False</td>\n",
       "      <td>['ester', 'lamothe', 'ficci√≥n', 'villo', 'ac√°'...</td>\n",
       "      <td>ester lamothe ficci√≥n villo ac√° comedia pol√≠ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.4349</td>\n",
       "      <td>pobre, servir, √©l, comida, ten√©s, culpa, onda,...</td>\n",
       "      <td>1</td>\n",
       "      <td>hgw6zvd</td>\n",
       "      <td>Memeüí©</td>\n",
       "      <td>0</td>\n",
       "      <td>Eso porque son todos √∫tos chupa bija.. Venga e...</td>\n",
       "      <td>hgw2528</td>\n",
       "      <td>True</td>\n",
       "      <td>['√∫to', 'chupa', 'bijo', 'venir', 'ban', 'nedf...</td>\n",
       "      <td>√∫to chupa bijo venir ban nedflanducacion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.8089</td>\n",
       "      <td>re, cabeza, √©l, morir, pibes, papa, hambre, ri...</td>\n",
       "      <td>1</td>\n",
       "      <td>hgw24ns</td>\n",
       "      <td>Memeüí©</td>\n",
       "      <td>0</td>\n",
       "      <td>mas verso burgu√©s que Maximo no hay. Es la rep...</td>\n",
       "      <td>q9hut7</td>\n",
       "      <td>False</td>\n",
       "      <td>['verso', 'burgu√©s', 'maximo', 'representaci√≥n']</td>\n",
       "      <td>verso burgu√©s maximo representaci√≥n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.3722</td>\n",
       "      <td>√©l, recordar, pegar, √∫nico, robar, barrio, pen...</td>\n",
       "      <td>1</td>\n",
       "      <td>hgw38x8</td>\n",
       "      <td>Memeüí©</td>\n",
       "      <td>0</td>\n",
       "      <td>Ayudar con comida? Na mejor unos afiches a tod...</td>\n",
       "      <td>q9hut7</td>\n",
       "      <td>False</td>\n",
       "      <td>['ayudar', 'comida', 'na', 'afich', 'color']</td>\n",
       "      <td>ayudar comida na afich color</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.3461</td>\n",
       "      <td>the, of, necesitar, you, f√°cil, and, to, creer...</td>\n",
       "      <td>1</td>\n",
       "      <td>hgw2rml</td>\n",
       "      <td>Memeüí©</td>\n",
       "      <td>1</td>\n",
       "      <td>¬øPor qu√© si es un cerdo tiene 6 patas?</td>\n",
       "      <td>q9hut7</td>\n",
       "      <td>False</td>\n",
       "      <td>['cerdo', 'pata']</td>\n",
       "      <td>cerdo pata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.3515</td>\n",
       "      <td>ah, /s, peronista, paso, x200b, mes, cagar, √©l...</td>\n",
       "      <td>1</td>\n",
       "      <td>hgw3wei</td>\n",
       "      <td>Memeüí©</td>\n",
       "      <td>0</td>\n",
       "      <td>Mira, soy tan capitalista que por 15 mil pesos...</td>\n",
       "      <td>q9hut7</td>\n",
       "      <td>False</td>\n",
       "      <td>['mira', 'capitalisto', 'pesos', 'corrijo', 'c...</td>\n",
       "      <td>mira capitalisto pesos corrijo color</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.4082</td>\n",
       "      <td>milei, pasar, debate, votar, voto, mujer, izqu...</td>\n",
       "      <td>1</td>\n",
       "      <td>hgw78bv</td>\n",
       "      <td>Memeüí©</td>\n",
       "      <td>0</td>\n",
       "      <td>Swinetaur libertario de Darkest Per√≥nia. Ruin ...</td>\n",
       "      <td>q9hut7</td>\n",
       "      <td>False</td>\n",
       "      <td>['swinetaur', 'libertario', 'darkest', 'per√≥ni...</td>\n",
       "      <td>swinetaur libertario darkest per√≥nia ruin come...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.4081</td>\n",
       "      <td>√©l, foto, ver, libertad, sacar, feriado, tomar...</td>\n",
       "      <td>1</td>\n",
       "      <td>hgw6rim</td>\n",
       "      <td>Memeüí©</td>\n",
       "      <td>0</td>\n",
       "      <td>como no pueden contra elllll. lo ensucian vamo...</td>\n",
       "      <td>q9hut7</td>\n",
       "      <td>False</td>\n",
       "      <td>['elllll', 'ensuciar', 'milie', 'bastar', 'k']</td>\n",
       "      <td>elllll ensuciar milie bastar k</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Dominant_Topic  Perc_Contribution  \\\n",
       "0      0            12.0             0.2531   \n",
       "1      1             2.0             0.2598   \n",
       "2      2            26.0             0.3279   \n",
       "3      3             6.0             0.4349   \n",
       "4      4            21.0             0.8089   \n",
       "5      5            12.0             0.3722   \n",
       "6      6            17.0             0.3461   \n",
       "7      7            22.0             0.3515   \n",
       "8      8            24.0             0.4082   \n",
       "9      9            23.0             0.4081   \n",
       "\n",
       "                                      Topic_Keywords  score       id  \\\n",
       "0  √©l, recordar, pegar, √∫nico, robar, barrio, pen...      1  hgw77qe   \n",
       "1  perro, nik, meme, gobierno, explicar, it, teni...      1  hgw7dci   \n",
       "2  falacia, decir, gratis, k, joda, pa√≠s, mandar,...      1  hgw69er   \n",
       "3  pobre, servir, √©l, comida, ten√©s, culpa, onda,...      1  hgw6zvd   \n",
       "4  re, cabeza, √©l, morir, pibes, papa, hambre, ri...      1  hgw24ns   \n",
       "5  √©l, recordar, pegar, √∫nico, robar, barrio, pen...      1  hgw38x8   \n",
       "6  the, of, necesitar, you, f√°cil, and, to, creer...      1  hgw2rml   \n",
       "7  ah, /s, peronista, paso, x200b, mes, cagar, √©l...      1  hgw3wei   \n",
       "8  milei, pasar, debate, votar, voto, mujer, izqu...      1  hgw78bv   \n",
       "9  √©l, foto, ver, libertad, sacar, feriado, tomar...      1  hgw6rim   \n",
       "\n",
       "             flair  comms_num  \\\n",
       "0       Pol√≠ticaüèõÔ∏è          0   \n",
       "1       Pol√≠ticaüèõÔ∏è          0   \n",
       "2  Humor:snoo_joy:          0   \n",
       "3            Memeüí©          0   \n",
       "4            Memeüí©          0   \n",
       "5            Memeüí©          0   \n",
       "6            Memeüí©          1   \n",
       "7            Memeüí©          0   \n",
       "8            Memeüí©          0   \n",
       "9            Memeüí©          0   \n",
       "\n",
       "                                                body comment_parent_id  \\\n",
       "0  Iba a decir, bue si lo saco de su bolsillo... ...            q9imco   \n",
       "1                Se volvio un meme el bot del dolar?           hgw666m   \n",
       "2  Este Esteban Lamothe estaba en la ficci√≥n de u...            q9i4uj   \n",
       "3  Eso porque son todos √∫tos chupa bija.. Venga e...           hgw2528   \n",
       "4  mas verso burgu√©s que Maximo no hay. Es la rep...            q9hut7   \n",
       "5  Ayudar con comida? Na mejor unos afiches a tod...            q9hut7   \n",
       "6             ¬øPor qu√© si es un cerdo tiene 6 patas?            q9hut7   \n",
       "7  Mira, soy tan capitalista que por 15 mil pesos...            q9hut7   \n",
       "8  Swinetaur libertario de Darkest Per√≥nia. Ruin ...            q9hut7   \n",
       "9  como no pueden contra elllll. lo ensucian vamo...            q9hut7   \n",
       "\n",
       "   is_replay                                       lemma_tokens  \\\n",
       "0      False  ['bue', 'saco', 'bolsillo', 'recorder', 'hdp',...   \n",
       "1       True                        ['volvio', 'meme', 'dolar']   \n",
       "2      False  ['ester', 'lamothe', 'ficci√≥n', 'villo', 'ac√°'...   \n",
       "3       True  ['√∫to', 'chupa', 'bijo', 'venir', 'ban', 'nedf...   \n",
       "4      False   ['verso', 'burgu√©s', 'maximo', 'representaci√≥n']   \n",
       "5      False       ['ayudar', 'comida', 'na', 'afich', 'color']   \n",
       "6      False                                  ['cerdo', 'pata']   \n",
       "7      False  ['mira', 'capitalisto', 'pesos', 'corrijo', 'c...   \n",
       "8      False  ['swinetaur', 'libertario', 'darkest', 'per√≥ni...   \n",
       "9      False     ['elllll', 'ensuciar', 'milie', 'bastar', 'k']   \n",
       "\n",
       "                                  body_preprocessing  \n",
       "0  bue saco bolsillo recorder hdp mantener alcanz...  \n",
       "1                                  volvio meme dolar  \n",
       "2  ester lamothe ficci√≥n villo ac√° comedia pol√≠ti...  \n",
       "3           √∫to chupa bijo venir ban nedflanducacion  \n",
       "4                verso burgu√©s maximo representaci√≥n  \n",
       "5                       ayudar comida na afich color  \n",
       "6                                         cerdo pata  \n",
       "7               mira capitalisto pesos corrijo color  \n",
       "8  swinetaur libertario darkest per√≥nia ruin come...  \n",
       "9                     elllll ensuciar milie bastar k  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "#df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.to_csv(TEXT_SAVE_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}