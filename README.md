

# Vistazo r√°pido

El presente repo contiene el c√≥digo correspondiente al proyecto final de la materia [Miner√≠a de datos para texto](https://sites.google.com/unc.edu.ar/textmining2021/), a cargo de [Laura Alonso i Alemany](https://cs.famaf.unc.edu.ar/~laura/).

Objetivo del proyecto: Caracterizar discursos de odio dentro de la comunidad de [reddit Argentina](https://reddit.com/r/argentina). Esto es, detectarlos y encontrar sub-lenguajes de odio en los mismos.

Para realizar esto, se llev√≥ a cabo un proceso consistente en 6 etapas, como se muestra en la siguiente figura:

![pipeline_reddit](/misc/workflow.drawio.png)


Cada etapa tiene su correspondiente notebook:

1. Obtenci√≥n del conjunto de comentarios de a trav√©s de la API de Reddit ([link](https://github.com/PerseoSoft/redditHateSpeech/blob/main/src/1_pipeline_download_reddit_comments.ipynb)).
   
2. Pre-procesamiento del mismo ([link](https://github.com/PerseoSoft/redditHateSpeech/blob/main/src/2_pipeline_preprocessing.ipynb)).

3. Aplicaci√≥n de *embeddings* y categorizaci√≥n en *clusters* (*notebook* [LDA](https://github.com/PerseoSoft/redditHateSpeech/blob/main/src/3a_pipeline_lda.ipynb) [Word2vec](https://github.com/PerseoSoft/redditHateSpeech/blob/main/src/3b_pipeline_embedding_word2vec.ipynb) [fastText](https://github.com/PerseoSoft/redditHateSpeech/blob/main/src/3c_pipeline_embedding_fasttext.ipynb)).

4. Entrenamiento de un modelo de detecci√≥n de odio y extracci√≥n de palabras de odio en cada *dataset* ([link](https://github.com/PerseoSoft/redditHateSpeech/blob/main/src/4_detect_hate_speech.ipynb)).
Para realizar el entrenamiento de los modelos, es necesario contar con los *datasets* respectivos de tres competencias (Hateval, DETOXIS, MeOffendMex) que se desee entrenar.

5. Uso del modelo para predecir los comentarios recolectados ([link](https://github.com/PerseoSoft/redditHateSpeech/blob/main/src/5_pipeline_hate_speech.ipynb)).

6. Combinaci√≥n de dicho modelo con las categor√≠as encontradas para encontrar correlaciones ([link](https://github.com/PerseoSoft/redditHateSpeech/blob/main/src/6_pipeline_result.ipynb)).


## Instalaci√≥n

### Instalaci√≥n con conda

Instalar Anaconda ([ver aqu√≠](https://docs.anaconda.com/anaconda/install/index.html)) y luego ejecutar:

```bash
#Crear entorno con conda y activarlo
conda env create -f environment.yml
conda activate hateSpeech
#Descarga del Trained pipelines de spaCy
python -m spacy download es_core_news_lg
#Correr Jupyter Lab
jupyter lab --ip=0.0.0.0 --no-browser --allow-root --NotebookApp.token='' --NotebookApp.password=''
```
Ir a [http://localhost:8888](http://localhost:8888) para acceder a la UI de Jupyter.

### Instalaci√≥n con Docker Compose

Instalar Docker Compose ([ver aqu√≠](https://docs.docker.com/compose/install/)) y luego ejecutar:

```bash
#Construir imagen
docker-compose build
#Correr Jupyter Lab
docker-compose up -d
```

Ir a [http://localhost:8888](http://localhost:8888) para acceder a la UI de Jupyter.

## Flujo de datos generados

Los distintos notebooks forman un pipeline en el cu√°l cada uno utiliza los datos generados por el anterior. Se listan cada una de las entradas:

1. Obtenci√≥n de comentarios. 
    - Archivos de entrada: N/A. 
    - Archivo de salida: *docs/reddit_data.csv*: CSV que contiene los comentarios de reddit descargados

2. Pre-procesamiento del dataset.
    - Archivos de entrada: *docs/reddit_data.csv*.
    - Archivos de salida: *docs/preprocessing_reddit_data.csv*: CSV con los comentarios pre-procesados.
   

3. Embeddings y clustering.
    - Archivos de entrada: *docs/preprocessing_reddit_data.csv*.
    - Archivos de salida: 
      - *docs/reddit_data_METODO.csv*, donde *METODO* puede ser 'lda', o 'word2vec', 'fasttext'. Cada uno de estos archivos toma el dataset pre-procesado y le agrega el n√∫mero de cl√∫ster al que pertenecer√≠a cada comentario, seg√∫n su cercan√≠a.
      - *docs/models/MODEL.model*, el modelo entrenado. Puede ser 'word2vec', o 'fasttext'. 
      - *docs/models/MODEL_kmeans.model*, el modelo de k-means entrenado usando los embeddings de *MODEL* (para 'word2vec' y 'fasttext').

    
4. Entrenamiento y selecci√≥n del modelo.
   - Archivos de entrada: *docs/hateval2019/hateval2019_es_train.csv*, *docs/detoxis_data/train.csv*, y *docs/MeOffendEs/mx-train-data-non-contextual.csv*. Estos archivos requieren la descarga previa manual de cada dataset.
   - Archivos de salida: para cada dataset, se guarda:
     - Palabras de odio de cada modelo: *docs/palabras_odio.csv*.
     - Vectorizador: *docs/models/DATASET_vectorizer.pkl* donde *DATASET* es hateval, detoxis, o meoffendmex.
     - Modelo entrenado: *docs/models/DATASET_INICIALES_MODELO_model.pkl* donde *INICIALES_MODELO* es 'lr', 'rf', o 'nb'.
   - Archivos de salida (de prueba): Predicciones: *docs/test/reddit_DATASET_hate_comments.csv*, uno para cada *DATASET*: 'hateval', 'detoxis', 'meoffendmex'.
   
5. Aplicaci√≥n del modelo en comentarios de reddit. 
   - Archivos de entrada: *docs/reddit_data_METODO.csv*.
   - Archivos de salida:
     - *docs/reddit_data_hate_speech_METODO.csv* - CSV que categoriza cada uno de los comentarios como de odio/no odio.
6. An√°lisis de resultados.
   - Archivos de entrada: 
     * *docs/reddit_data_hate_speech_METODO.csv*
     * *docs/palabras_odio.csv*
   - Archivos de Salida: N/A. 

---

# Informe del proyecto

Se muestra a continuaci√≥n el informe producto de este proyecto, en donde se especifican la motivaci√≥n y objetivos del trabajo, y los distintos enfoques abordados para realizar la detecci√≥n de odio.

√çndice

- [Vistazo r√°pido](#vistazo-r√°pido)
  - [Instalaci√≥n](#instalaci√≥n)
    - [Instalaci√≥n con conda](#instalaci√≥n-con-conda)
    - [Instalaci√≥n con Docker Compose](#instalaci√≥n-con-docker-compose)
  - [Flujo de datos generados](#flujo-de-datos-generados)
- [Informe del proyecto](#informe-del-proyecto)
  - [Introducci√≥n](#introducci√≥n)
    - [Discursos de odio](#discursos-de-odio)
    - [Motivaci√≥n del trabajo](#motivaci√≥n-del-trabajo)
    - [reddit](#reddit)
      - [¬øPor qu√© r/argentina?](#por-qu√©-rargentina)
  - [Paso a paso del proyecto](#paso-a-paso-del-proyecto)
    - [1. Obtenci√≥n de los datos](#1-obtenci√≥n-de-los-datos)
    - [2. Pre-procesamiento](#2-pre-procesamiento)
    - [3. Representaci√≥n de t√≥picos mediante embeddings](#3-representaci√≥n-de-t√≥picos-mediante-embeddings)
      - [3a. Embeddings con LDA](#3a-embeddings-con-lda)
    - [3b. Embeddings con Word2vec](#3b-embeddings-con-word2vec)
    - [3c. Embeddings con fastText](#3c-embeddings-con-fasttext)
  - [4. Entrenamiento de detectores de odio](#4-entrenamiento-de-detectores-de-odio)
  - [5. Aplicaci√≥n del modelo a los comentarios de reddit](#5-aplicaci√≥n-del-modelo-a-los-comentarios-de-reddit)
  - [6. An√°lisis de resultados](#6-an√°lisis-de-resultados)
    - [6.1. Vista general de los distintos clusters](#61-vista-general-de-los-distintos-clusters)
    - [6.2. Vista de los clusters con mayor proporci√≥n de predicci√≥n positiva](#62-vista-de-los-clusters-con-mayor-proporci√≥n-de-predicci√≥n-positiva)
    - [6.3. Detecci√≥n de cl√∫sters seg√∫n palabras asociadas con odio](#63-detecci√≥n-de-cl√∫sters-seg√∫n-palabras-asociadas-con-odio)
    - [6.4. An√°lisis cercano de dos cl√∫sters](#64-an√°lisis-cercano-de-dos-cl√∫sters)
      - [Cluster de G√©nero](#cluster-de-g√©nero)
      - [Cluster de Soberan√≠a](#cluster-de-soberan√≠a)
    - [6.5. Visualizaci√≥n de t√©rminos cercanos seleccionados](#65-visualizaci√≥n-de-t√©rminos-cercanos-seleccionados)
  - [Conclusiones](#conclusiones)
  - [Trabajo futuro](#trabajo-futuro)
    - [General](#general)
    - [Clustering](#clustering)
    - [Modelo](#modelo)
    - [Informaci√≥n de contexto](#informaci√≥n-de-contexto)
  - [Fuentes consultadas para el trabajo](#fuentes-consultadas-para-el-trabajo)
    - [Discursos de odio](#discursos-de-odio-1)
    - [reddit API](#reddit-api)
    - [Procesamiento de lenguaje natural](#procesamiento-de-lenguaje-natural)
    - [Clustering](#clustering-1)
    - [Competencias](#competencias)
    - [Trabajos relacionados](#trabajos-relacionados)


## Introducci√≥n

### Discursos de odio

El discurso de odio es un problema muy relevante en la actualidad, dado su rol en la discriminaci√≥n de grupos y minor√≠as sociales, y [es considerado como precursor de cr√≠menes de odio](https://www.rightsforpeace.org/hate-speech) [que incluyen al genocidio](https://scholarcommons.usf.edu/gsp/vol7/iss1/4).

Hay varias posturas sobre lo que es el discurso de odio, en general se coincide en que es un discurso que:

1. Apunta contra un grupo o individuo, basado en alg√∫n aspecto como su orientaci√≥n sexual, religi√≥n, nacionalidad, etc.
2. Busca humillar, discriminar o propagar el odio/hostilidad/intolerancia hacia ese grupo.
3. Tiene una intenci√≥n deliberada.

Su manifestaci√≥n en Internet, adem√°s:

1. Puede motivar formas de agresi√≥n en l√≠nea.
2. Permite propagar el discurso de odio con velocidad.
3. Permite que el discurso se mantenga y comparta con facilidad.
4. Facilita la generaci√≥n de c√°maras de eco.
5. Al estar en servidores privados, la aplicaci√≥n de la ley no siempre es r√°pida, lo que hace que ciertos actores intenten eludir su control, utilizando el discurso de odio en beneficio de su agenda.

A ra√≠z de la gravedad que significa el problema, muchas plataformas sociales han reconocido el problema, tomando acciones para mitigarlo ([ver ejemplo](https://www.theguardian.com/technology/2016/may/31/facebook-youtube-twitter-microsoft-eu-hate-speech-code)),  prohibiendolo en sus t√©rminos de uso, pudiendo sus usuarios reportar comentarios que potencialmente contengan este tipo de discursos.
No obstante, a pesar de las prohibiciones y esfuerzos, que hasta llegan a incluir algoritmos de detecci√≥n autom√°tica de discursos de odio en plataformas como Facebook e Instagram, el problema de la propagaci√≥n de odio en redes sociales persiste, y genera da√±o, tanto a individuos como a comunidades.


### Motivaci√≥n del trabajo

Considerando las consecuencias que pueden traer aparejadas los discursos de odio, este trabajo se enfoca en la detecci√≥n de tales discursos en una comunidad particular de reddit. Los objetivos del mismo son: **1)** detecci√≥n de comentarios con discurso de odio y **2)** caracterizar ese discurso de odio en sub-lenguajes de odio.

El presente trabajo se basa en la siguiente hip√≥tesis: *"en una comunidad en donde existen comentarios con discurso de odio, es beneficioso combinar t√©cnicas de aprendizaje supervisado y no supervisado, para realizar la detecci√≥n de subcomunidades de odio, a partir de modelos que se especializan en distintos grupos de comentarios"*.

### reddit

[Reddit](https://www.reddit.com/) es una red social de ‚Äúcomunidades‚Äù, creadas y moderadas por sus propios usuarios. En cada comunidad, sus miembros hacen *posts*, y cada *post* puede ser comentado generando debate. Su aspecto distintivo es que cada *post* o comentario recibe votos, con el objetivo de que aquellos *posts* o comentarios que m√°s aportan aparezcan encima de los que no. Tambi√©n se pueden premiar a aquellos destacados. 

En la siguiente imagen podemos ver la estructura general de un *post* en reddit (de r/argentina):

![](misc/reddit.png)


En este proyecto, nos centramos en [r/argentina](https://www.reddit.com/r/argentina/), que es una comunidad dedicada a charlar temas referentes a Argentina, que incluyen comidas, costumbres, chistes, deporte, pol√≠tica,  econom√≠a, consejos, entre otros.

#### ¬øPor qu√© r/argentina?

Quisimos hacer nuestro trabajo enfocado en una comunidad Argentina fuera de las redes sociales m√°s comunes (dado que son aquellas m√°s frecuentemente estudiadas), pero que a la vez tenga el tama√±o suficiente como para tener muchos usuarios e interacciones. En ese sentido, r/argentina fue la opci√≥n m√°s prominente, ya que la comunidad es muy activa y cuenta con cerca de 350.000 suscriptores (a Noviembre de 2021).

Respecto a su posici√≥n frente a discursos de odio, en las reglas de r/argentina (en concreto, la Regla 3) se deja totalmente de manifiesto su prohibici√≥n. Citando textualmente:

>**3. No se permite el racismo, xenofobia u otras expresiones de odio**
>
> No se permite el racismo, xenofobia, ni ninguna otra forma de odio (incluyendo sexismo, homofobia, transfobia, clase social, etc), ni ning√∫n tipo de discriminaci√≥n o expresiones de odio o lenguaje deshumanizante en general; esto incluye comentarios incitando violencia. Esto tambi√©n se extiende a grupos. Hacer referencia a enfermedades o discapacidades para insultar a otros no ser√° tolerado. Usuarios que incurran en estas faltas podr√°n ser baneados permanentemente sin apelaci√≥n.


No obstante, al elaborar este trabajo, hemos detectado casos de comentarios con discursos de odio, ej.: manifestando [aporofobia](https://es.wikipedia.org/wiki/Aporofobia), [obesofobia](https://es.wikipedia.org/wiki/Obesofobia), o comentarios agresivos contra mujeres, entre otros.

Dada esta situaci√≥n, la motivaci√≥n de nuestro trabajo es la de poder detectar autom√°ticamente este tipo de comentarios, pudiendo caracterizar los mismos en sub-comunidades.


## Paso a paso del proyecto

Se describe a continuaci√≥n, el paso a paso de las distintas etapas de este proyecto, partiendo de los datos iniciales, c√≥mo los mismos fueron procesados y usados para entrenar distintos algoritmos, los resultados obtenidos tras ello, y finalmente las conclusiones y trabajo futuro.


### 1. Obtenci√≥n de los datos

[Notebook](/src/1_pipeline_download_reddit_comments.ipynb)

Para la obtenci√≥n de los datos se utiliz√≥ un *wrapper* de la API de reddit, llamado [PRAW](https://praw.readthedocs.io/en/stable/index.html), a partir del cual se descargaron comentarios de diferentes *post* del r/argentina, as√≠ como las respuestas de los comentarios.
Los *posts* en reddit pueden ser de tipo *link* (por ejemplo, colocando el *link* hacia una noticia), o pueden ser de tipo texto.
Para la descarga de comentarios de cada *post*, se consideraron s√≥lo aquellos que conten√≠an texto, y una cierta cantidad de caracteres como m√≠nimo.

De cada comentario que se guard√≥ de reddit, se obtuvieron los siguientes datos:
- **id**: identificador del *post* o comentario. Guardado por cuestiones de trazabilidad.
- **comment_parent_id**: identificador del comentario al cu√°l responde el comentario actual, en caso que corresponda. Se guard√≥ por cuestiones de trazabilidad.
- **flair**: categor√≠a del *post*, asignada por el usuario que lo crea (a partir de una lista brindada por el propio subreddit). En el caso de r/argentina, las categor√≠as incluyen t√≥picos como "Pol√≠tica", "Econom√≠a", "Humor", "Historia" o "Serio".
- **comms_num**: n√∫mero de respuestas que recibi√≥ el comentario.
- **score**: es un puntaje que los usuarios le dieron al comentario.

En total, se descargaron 27791 comentarios desde el d√≠a 08/10/2021 hasta el 19/10/2021.


### 2. Pre-procesamiento

[Notebook](/src/2_pipeline_preprocessing.ipynb)

Teniendo descargados los datos, se aplic√≥ un pre-procesamiento sobre cada comentario, que consisti√≥ en:

- Eliminar emojis, urls, comillas, caracteres especiales y puntuaciones.
- Aplicar tokenizaci√≥n, dividiendo cada comentario en sus correspondientes palabras.
- Conversi√≥n a min√∫scula.
- Eliminaci√≥n de *stopwords* (mediante spaCy).
- Lematizaci√≥n (mediante spaCy).
- Construir bigramas y trigramas.

### 3. Representaci√≥n de t√≥picos mediante *embeddings*

Teniendo los comentarios pre-procesados, el siguiente objetivo fue detectar t√≥picos a partir de los mismos de acuerdo a las co-ocurrencias de las palabras, para poder identificar los distintos temas que se hablan, y los sublenguajes empleados en ellos.

Para poder llevar esto a cabo, se emplearon tres m√©todos en los datos obtenidos:

1. Latent Dirichlet Allocation.
2. Word2vec.
3. fastText.

Se describe a continuaci√≥n cada uno de ellos, mostrando particularmente algunos comentarios que fueron agrupados a trav√©s de las diferentes t√©cnicas aplicadas. 
Analizamos un evento particular que se encuentra presente en los tres m√©todos, y se captur√≥ durante la descarga de estos datos.
Este evento fue el debate de la "[Ley de Promoci√≥n de la Alimentaci√≥n Saludable](https://www.boletinoficial.gob.ar/detalleAviso/primera/252728/20211112)", tambi√©n conocida como "ley de etiquetado frontal".
Vamos a comparar las subcomunidades obtenidas en cada t√©cnica, analizando particularmente aqu√©llas referidas a este evento.


#### 3a. *Embeddings* con LDA

[Notebook](/src/3a_pipeline_lda.ipynb)

El primer modelo que se comenz√≥ utilizando es [Latent Dirichlet Allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation), que es un m√©todo generativo que asume que cada documento est√° compuesto por una mezcla de t√≥picos, y donde cada palabra tiene una probabilidad de relacionarse con cada uno de ellos.
La elecci√≥n inicial de LDA se fundament√≥ en que es un m√©todo s√≥lido para detecci√≥n de t√≥picos en *corpus* de texto.

El modelo se aplic√≥ probando tama√±os de *cl√∫sters* de 30 a 120, y distintas configuraciones de h√≠per-par√°metros. No obstante, los resultados obtenidos  no fueron satisfactorios, ya que a la hora de realizar un an√°lisis de los t√≥picos identificados por el modelo, se encontr√≥ poca cohesi√≥n entre los t√≥picos detectados.

En la siguiente imagen se pueden observar algunos de los t√≥picos identificados por LDA.

![](misc/embedding_1.png)

El t√≥pico n√∫mero 91, **piedra - etiqueta - pan - mira**, incluye comentarios sobre la tratativa de la ley de etiquetado y temas que tienen que ver con la comida en general. Algunos comentarios son:

1. "Me alegro mucho, seguro muy feliz todos por el reencuentro. Igual te recomiendo que no coma directo de la lata, pasale a un platito o comedero. Entiendo que a veces ni te dan tiempo."
2. "Todo mi secundario el desayuno fue un fantoche triple y una lata de coca.  Y s√≥lo gastaba 2. Qu√© buenos tiempos."
3. "La manteca no hace mal. Es muy dif√≠cil comer exceso de grasas para tu cuerpo en comparaci√≥n con lo f√°cil que es atiborrarte con az√∫car y carbohidratos. Esos son los verdaderos enemigos"
4. "Y con etiquetas que te dicen cu√°nta grasa tiene un kilo de bayonesa"
5. "Alta banfest se van a mandar los mods con este thread. Despedite de tu cuenta, maquinola, denunciado"


### 3b. *Embeddings* con Word2vec

[Notebook](/src/3b_pipeline_embedding_word2vec.ipynb)

Dado que el funcionamiento con LDA no se consider√≥ como satisfactorio, el siguiente paso consisti√≥ probar otro tipo de modelos: los *embeddings* de palabras.
Los mismos consisten en llevar las palabras a un nuevo espacio, de forma tal que aquellas que comparten un contexto com√∫n en los comentarios obtenidos, tiendan a encontrarse mucho m√°s cerca que aquellas que no.
De esta manera, se podr√≠an identificar subcomunidades en este nuevo espacio.

Para ello, se llevaron a cabo los siguientes pasos:

1. Entrenar el modelo de generaci√≥n de *embeddings* de palabras mediante una **tarea de pretexto** (dada una palabra, predecir informaci√≥n relacionada a su contexto, por ejemplo una palabra que le sigue). Se emplearon dos modelos: [Word2vec](https://en.wikipedia.org/wiki/Word2vec), cuyos resultados se muestran en esta secci√≥n, y [fastText](https://en.wikipedia.org/wiki/fastText), mostrado en la siguiente.
2. Una vez entrenados los modelos, se procedi√≥ a generar una representaci√≥n vectorial de cada comentario, donde cada uno se mape√≥ a un vector num√©rico de acuerdo al promedio de los *embeddings* de cada una de sus palabras.
3. Se aplic√≥ el algoritmo de *clustering* *[k-means](https://en.wikipedia.org/wiki/K-means_clustering)*, tomando los vectores generados en el paso anterior.

Tras realizar el entrenamiento y aplicar *clustering*, se observ√≥ que los t√≥picos obtenidos se identificaban de forma mucho mejor que al usar LDA.
Estos t√≥picos, adem√°s se identificaron mejor con un n√∫mero alto de *clusters* (120), frente a un n√∫mero menor (como 30 o 70).
En la siguiente imagen se pueden observar algunas de las subcomunidades identificadas tras aplicar Word2vec.


![](misc/embedding_2.png)

En particular, el *cluster* n√∫mero 94, **ley - etiquetado - proyecto**, es el que incluye comentarios sobre la tratativa de la ley de etiquetado y temas que tienen que ver con las leyes en general. Algunos comentarios del mismo son:

1. "Una prueba mas de la ley de oferta y demanda"
2. "Con la nueva ley no le pod√©s regalar leche entera o un alfajor a un comedor, decir comida basura en un pa√≠s donde el 50\% de los chicos no hacen toda las comidas es lo m√°s clasista que existe."
3. "Recuerden la ley de alquileres.... Fu√© sancionada con un beso muy fuerte de los K, PRO y dem√°s muchachos..."
4. "No entiendo c√≥mo hay tanta gente en contra de una ley que no te cambia un carajo tu vida. Es la ley m√°s anodina que sac√≥ el Kirchnerismo en toda su historia creo"
5. "Pero hay leyes contra la violencia de genero! Como paso esto!!!1!?"
6. "No existe tal cosa en Argentina. Existe el Estado de Sitio, pero no se asemeja para nada a una ley marcial.. El concepto de ley marcial como tal, desapareci√≥ en el 94 con la nueva Constituci√≥n."

### 3c. *Embeddings* con fastText

[Notebook](/src/3c_pipeline_embedding_fasttext.ipynb)

Finalmente, el √∫ltimo m√©todo aplicado fue [fastText](https://en.wikipedia.org/wiki/fastText) que entrena una tarea de pretexto para generar un *embedding* de palabras al igual que Word2vec, pero adem√°s tiene en cuenta las sub-palabras, lo cu√°l resulta √∫til para identificar las alteraciones que puede tener una misma palabra.

En la siguiente imagen se pueden observar algunas de las subcomunidades identificadas por fastText.

![](misc/embedding_3.png)

Como se puede ver en el cluster **jaja - jajaja - jajajar - jajajaja - jajaj**, fastText identifica mejor las alteraciones que pueden suceder dentro de una palabra.

El *cluster* n√∫mero 113, **ley - etiquetado - votar**, incluye comentarios sobre la tratativa de la ley de etiquetado y temas que tienen que ver con las leyes en general. Algunos comentarios son:

1. "Feriado con fines tur√≠sticos. Ley 27.399"
2. "ajajaja como los cagaron a los primeros. como siempre la ley aplica a todos por igual /s"
3. "El sticker en Chile fue durante la transici√≥n de la ley. Imag√≠nate tener productos fabricados y tener que cambiar la envoltura a todos para que cumplan la ley"
4. "Gracias gloriosa ley de regulaci√≥n de alimentos, ahora se que desayunar coca cola con surtidos bagleys esta mal"
5. "Eso y que la ley va a prohibir vender dulces y gaseosas en los colegios, y usar im√°genes de famosos en los envases."
6. "Eso est√° por la ley Micaela no?. Tipo esta clase de capacitaciones no?"
7. "y ahora Lipovetzky reconoce lo de la ley de alquileres"

Si bien existen algunos *clusters* que nos permiten identificar t√≥picos espec√≠ficos (como el 113), se observ√≥ que si bien el m√©todo detecta variantes de palabras, en t√©rminos generales los *clusters* no se traducen en t√≥picos cohesivos. Por ejemplo, en el *cluster* n√∫mero 54 encontramos comentarios de diferentes t√≥picos:
1. "No lo veo a Belgrano? Saavedra?. Me re mintieron!"
2. "Mate de cafe re copado, un litro de cafe en tu organismo"
3. "ajajajajajaj Geologia, es re linda carrera igual pero esta materia es una completa mierda"
4. "cuando dije eso? milei est√° re bajon desde el debate del otro dia, me lo dice gente que habla con el casi todos los dias"

Tambi√©n se observ√≥ que algunos *clusters* se construyen exclusivamente alrededor de una palabra y sus variantes, por ejemplo el 43 se construy√≥ alrededor de la palabra **decir**:

1. "Por eso dije ""en general"". Hay excepciones."
2. "Son los muy menos. Yo dir√≠a que 1 de cada 100."
3. "6! Seis! Seis, por favor! Dije seissss??!!"
4. "sera lo que el gobierno diga"
5. "Lo s√© lo s√©... Me lo dec√≠a mi abuela"

Observando esto, y el buen rendimiento obtenido al usar Word2vec, se opt√≥ finalmente por avanzar en la identificaci√≥n de subcomunidades empleando dicha t√©cnica.


## 4. Entrenamiento de detectores de odio

[Notebook](/src/4_detect_hate_speech.ipynb)


En paralelo a la b√∫squeda de *clusters* que agrupan los distintos t√≥picos, se busc√≥ tambi√©n, a partir de los datos [pre-procesados anteriormente](#2-pre-procesamiento), el detectar autom√°ticamente comentarios de odio, para poder combinarlos con los [t√≥picos encontrados](#3-embeddings). Para ello, se recurri√≥ a conjuntos de datos anotados y en castellano, que hayan sido utilizados para tareas similares. En particular, se opt√≥ por los siguientes tres:


1. HatEval: *dataset* con cerca de 7000 *tweets* de usuarios de Espa√±a, que potencialmente manifiestan discurso de odio contra mujeres o inmigrantes. Este *dataset* es el m√°s parecido a la tarea que queremos resolver, ya que tiene datos etiquetados que marcan directamente si se trata o no de un *tweet* con discurso de odio, sea contra un individuo o un grupo.

2. DETOXIS: *dataset* con cerca de 3500 comentarios de sitios de noticias/foros espa√±oles, que posiblemente contienen toxicidad. Si bien un mensaje con toxicidad no es necesariamente discurso de odio (y un mensaje con discurso de odio puede tener toxicidad o no), suele estar asociado al mismo.

3. MeOffendMex: *dataset* con alrededor de 5000 *tweets* de usuarios de M√©xico, que posiblemente contienen mensajes ofensivos. Al igual que la toxicidad, un mensaje ofensivo no necesariamente est√° manifestando odio, pero suelen estar asociados. 

En cada uno de los mismos, se entrenaron tres modelos de aprendizaje supervisado: *[regresi√≥n log√≠stica](https://en.wikipedia.org/wiki/Logistic_regression)*, *[naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)* y *[random forest](https://en.wikipedia.org/wiki/Random_forests)*, todos provistos por la librer√≠a [scikit-learn](https://scikit-learn.org).


Para realizar el entrenamiento, a cada comentario se le aplic√≥ el vectorizador [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), que transform√≥ cada comentario en una matriz *sparse*, donde cada fila representa un comentario, y cada columna incluye las distintas palabras (unigramas) o combinaciones de dos o tres palabras (bigramas y trigramas).

Tal matriz, junto con las correspondientes etiquetas de cada comentario, constituyeron la entrada de cada uno de los modelos. Tales modelos funcionaron bastante bien con sus configuraciones b√°sicas, mostrando matrices de confusi√≥n s√≥lidas en los conjuntos de validaci√≥n, para las tareas para los que fueron entrenados.
La siguiente tabla muestra un vistazo r√°pido de los resultados obtenidos, al evaluar el rendimiento de los modelos en el conjunto de validaci√≥n.

| Modelo              | Dataset     | Tasa de aciertos | F1 clase 0 | F1 clase 1 |
|---------------------|-------------|:-----------------:|:----------:|:----------:|
| Regresi√≥n log√≠stica | Hateval     |        .8        |     .84    |     .76    |
| Naive Bayes         |             |        .8        |     .84    |     .75    |
| Random forest       |             |        .77       |     .79    |     .74    |
| Regresi√≥n log√≠stica | DETOXIS     |        .96       |     .98    |     .09    |
| Naive Bayes         |             |        .86       |     .92    |     .11    |
| Random forest       |             |        .96       |     .98    |     .00    |
| Regresi√≥n log√≠stica | MeOffendMex |        .77       |     .85    |     .53    |
| Naive Bayes         |             |        .76       |     .82    |     .62    |
| Random forest       |             |        .79       |     .87    |     .53    |

Una vez teniendo los modelos entrenados, se extrajeron las palabras que m√°s probablemente manifiesten odio en cada *dataset*, de acuerdo a su aporte a la clasificaci√≥n de las palabras. Estas palabras fueron guardadas en el archivo [palabras_odio.csv](src/docs/palabras_odio.csv).
Por otra parte, [se guardaron](src/docs/models/) los modelos entrenados en cada *dataset*, y sus correspondientes vectorizaciones para su uso posterior.


## 5. Aplicaci√≥n del modelo a los comentarios de reddit

[Notebook](/src/5_pipeline_hate_speech.ipynb)

Teniendo los modelos entrenados en tres *datasets* con tareas similares, el siguiente paso consisti√≥ en aplicarlos en los comentarios recolectados y [preprocesados previamente](#2-pre-procesamiento), para evaluar c√≥mo los mismos se desenvuelven, viendo algunos de los comentarios que fueron predichos como positivos.
Lo primero que se observ√≥ al verlos fue la significativa cantidad de falsos positivos (con el umbral de clasificaci√≥n por defecto de 50\%), prediciendo como verdaderos a comentarios totalmente inofensivos.

A ra√≠z de ello, se opt√≥ por incrementar los umbrales de clasificaci√≥n de los modelos en pos de reducir los falsos positivos. En la siguiente tabla se observa c√≥mo var√≠a la cantidad de comentarios clasificados como positivos de acuerdo al umbral de clasificaci√≥n, de los 27791 comentarios recolectados en total.

| Modelo              | Dataset     | # pred. umb. 0.5 | # pred. umb. 0.6 | # pred. umb. 0.7 | # pred. umb. 0.8 | # pred. umb. 0.9 |
|---------------------|-------------|:----------------:|:----------------:|:----------------:|:----------------:|:----------------:|
| Regresi√≥n log√≠stica | HatEval     |       5344       |       3151       |       1710       |        790       |        227       |
| Naive Bayes         | HatEval     |       10420      |       6951       |       4312       |       2370       |        948       |
| Random forest       | HatEval     |       1336       |        338       |        52        |         3        |         0        |
| Regresi√≥n log√≠stica | DETOXIS     |        19        |         7        |         2        |         0        |         0        |
| Naive Bayes         | DETOXIS     |       3695       |       2393       |       1618       |       1003       |        512       |
| Random forest       | DETOXIS     |         0        |         0        |         0        |         0        |         0        |
| Regresi√≥n log√≠stica | MeOffendMex |       1197       |        679       |        367       |        166       |        50        |
| Naive Bayes         | MeOffendMex |       7977       |       5247       |       3502       |       2075       |       1080       |
| Random forest       | MeOffendMex |        455       |        167       |        72        |        14        |         0        |


De la misma, se ve que naive Bayes es el modelo que mayor cantidad de comentarios clasifica como positivo. Por ejemplo, en HatEval, empleando un umbral de 0.5, clasifica un 37\% del total de comentarios como de odio. A este modelo le siguen la regresi√≥n log√≠stica y random forest, que tiende a clasificar muy poca cantidad de ejemplos como positivo al aumentar el umbral desde 0.6 (llegando a no clasificar ning√∫n ejemplo como positivo en el dataset DETOXIS).

Por otra parte, se observ√≥ tambi√©n (en un vistazo general de las predicciones) que el *dataset* con el mejor rendimiento en la detecci√≥n fue MeOffendMex. Tenemos la conjetura de que esto se debe a que el lenguaje que de los comentarios de este *dataset* es el m√°s parecido al utilizado en r/argentina.


## 6. An√°lisis de resultados

[Notebook](/src/6_pipeline_result.ipynb)

En la siguiente secci√≥n, se toman los [*clusters* generados](#3-representaci√≥n-de-t√≥picos-mediante-embeddings), los [modelos entrenados](#4-entrenamiento-del-detector-de-odio) y [sus predicciones](#5-aplicaci√≥n-del-modelo-a-los-comentarios-de-reddit), para llevar a cabo un an√°lisis de los resultados obtenidos.

Para este an√°lisis, se us√≥ el modelo Naive Bayes (con un umbral de clasificaci√≥n de 0.8) entrenado sobre el conjunto de datos MeOffendMex, y el modelo Word2vec entrenado previamente.
Como en MeOffendMex cada comentario se etiqueta seg√∫n si representa una ofensa/agresi√≥n (y no necesariamente si es discurso de odio), consideramos a cada comentario como "positivo" si el mismo contiene una agresi√≥n o discurso de odio, y como "negativo" en caso contrario.
La raz√≥n de este criterio es que en los datos observados, la mayor√≠a de veces un comentario con discurso de odio incluye tambi√©n una agresi√≥n, y aunque no sucede lo mismo a la inversa, consideramos que es un buen punto de partida para abordarlo desde este trabajo.

### 6.1. Vista general de los distintos clusters

Vemos una vista general de los datos con los que se cuenta hasta ahora, con respecto a su distribuci√≥n en los distintos *clusters*.

* Se cuenta con 27.791 comentarios, donde cada uno tiene asignado un n√∫mero de t√≥pico y una etiqueta indicando si el clasificador lo categoriz√≥ como discurso de odio / agresivo o no. Los mensajes se distribuyen en los t√≥picos de la siguiente manera:

![](misc/num_topicos.png)

* En muchos de los clusters se identifican t√≥picos concretos. Algunos ejemplos:
  * *Cluster* 8: econom√≠a.
  * *Cluster* 18: pol√≠tica.
  * *Cluster* 23: d√≥lar.
  * *Cluster* 94: leyes.
  * *Cluster* 98: comidas.
  * *Cluster* 99: g√©nero.
  * *Cluster* 116: insultos.

* De los 27.791 comentarios, 2075 fueron predichos como de odio por el clasificador seleccionado. Tales predicciones se distribuyen como sigue:

![](misc/pred_hs_por_topico.png)

* De los distintos *clusters*, existen varios cuyo porcentaje de comentarios predicho como odio es muy significativo:

| N√∫mero de cluster | \% pred. positivas |
|:-----------------:|:-----------------------:|
|        116        |           73\%          |
|         66        |           39\%          |
|         79        |           36\%          |
|         27        |           27\%          |
|         93        |           24\%          |


* Vemos tambi√©n el porcentaje de comentarios predichos en cada *flair*:

|     Flair    | \% pred. positivas |
|:------------:|:-----------------------:|
|  Historiaüá¶üá∑  |         11\%        |
|  Policialesüö® |         10\%        |
|   Pol√≠ticaüèõÔ∏è  |         9\%        |
|   Meet-up‚ùó   |         9\%        |
|    VideoüìΩÔ∏è    |         9\%        |


* Vemos, para todos los *clusters* en general y para los tres de mayor proporci√≥n de predicciones en particular, si existe una correlaci√≥n lineal tanto entre el puntaje y la cantidad de r√©plicas de cada comentario, y su predicci√≥n como mensaje de odio.

| Cluster | Corr. puntaje y pred. pos. | Corr. num. com. y pred. pos. |
|:-------:|:--------------------------:|:----------------------------:|
|  Todos  |          -0.001         |           -0.016          |
|   116   |          0.028          |           0.005           |
|    66   |          0.068          |           0.170           |
|    79   |          -0.025          |           -0.150          |

Al ver todos los *clusters*, se observa que no existe una correlaci√≥n lineal entre el puntaje o cantidad de comentarios obtenidos, y clasificaci√≥n o no como discurso de odio. Por otra parte, al ver esto en los tres *clusters* donde se detect√≥ una mayor proporci√≥n de discurso de odio, se observa que la correlaci√≥n var√≠a levemente seg√∫n el caso, no habiendo encontrado un patr√≥n en esta variaci√≥n.


### 6.2. Vista de los *clusters* con mayor proporci√≥n de predicci√≥n positiva

Vemos los t√©rminos m√°s cercanos a los centroides de cada uno de los tres *clusters* con m√°s proporci√≥n de predicciones positivas (el 116, 66 y 79):

* *Cluster* 116: "hijo puta kjjjjjjjjjjj palm√≥ comper pobretonto pario colabor√° ramen vigote "
* *Cluster* 66: "va coquetar orina desmechado ansiosa amigoooo catre vas guita saf√°s "
* *Cluster* 79: "kjjjjjjjjjjj hijo palm√≥ comper pobretonto ahorcandolo pario puta ramen refuta "

Vemos ahora las palabras de mayor frecuencia (tanto predichas o no como odio), encontradas en los mismos.

![](misc/top_3_clusters_word_freq_1.png)
![](misc/top_3_clusters_word_freq_2.png)
![](misc/top_3_clusters_word_freq_3.png)

Puede observarse que se detectan muchos insultos en los tres *clusters*. No obstante, no se distingue una separaci√≥n clara de los t√©rminos usados (tanto de odio como de no odio) al realizar agrupamiento por t√©rminos m√°s frecuentes. Por ello, se opt√≥ por ordenarlos seg√∫n su [informaci√≥n mutua puntual](https://es.wikipedia.org/wiki/Punto_de_informaci%C3%B3n_mutua) (PMI). Se muestra abajo como quedar√≠an entonces los t√©rminos agrupados de esta forma, en donde se puede ver que el ordenamiento es mucho mejor:

![](misc/top_3_clusters_word_pmi_1.png)
![](misc/top_3_clusters_word_pmi_2.png)
![](misc/top_3_clusters_word_pmi_3.png)

Otro aspecto que se observa es que el puntaje promedio y la cantidad de respuestas que se reciben en los dos primeros *clusters* es mayor si los comentarios fueron clasificados como de odio/agresi√≥n.

### 6.3. Detecci√≥n de *clusters* seg√∫n palabras asociadas con odio

Dadas las palabras asociadas con odio extra√≠das anteriormente de los modelos, se analiza si es posible encontrar nuevos *clusters* que tengan contenido de agresi√≥n u odio, en base a la distancia de cada una de las mismas con respecto a ellos.
Para ello, se obtienen los *clusters* m√°s cercanos de cada una de dichas palabras, y se eval√∫a cu√°les fueron los *clusters* que ocurrieron m√°s frecuentemente al considerar todas las palabras.

El resultado se puede ver en la siguiente tabla:

| *Cluster* m√°s frecuente (# en top 1) | *Cluster* m√°s frecuente (# en top 3) |
|:----------------------------------:|:-------------------------------------------------:|
|            0 (72 veces)            |                    0 (72 veces)                   |
|              113 (10)              |                      87 (72)                      |
|               24 (4)               |                      86 (72)                      |
|               116 (4)              |                      113 (15)                     |
|               81 (1)               |                      24 (14)                      |

Vemos cu√°les fueron los t√©rminos m√°s comunes de cada *cluster* detectado. Respecto a los *clusters* que m√°s se repitieron:

* *Cluster* 0: "hacer dislocar desuscribite ss vtv paja preferir√≠a bosta oooon maloliente "
* *Cluster* 113: "ibarra baratisimo diz rayitar candadito feriar dolaaaar mote doxxeo gual "
* *Cluster* 24: "childrir changes clothes argument wage \-mr oooon boah pandemic ‚£Ñ "

Respecto a los *clusters* que m√°s aparecieron en entre los tres m√°s cercanos (excluyendo el *cluster* 0):

* *Cluster* 87: "macri sander bowie ionizante acuario galperin descubierto peluco preferio freestyler "
* *Cluster* 86: "salir biodegradar tenian grabate navegar pens√©s esfuenzar chango platea drogar "
* *Cluster* 113: "ibarra baratisimo diz rayitar candadito feriar dolaaaar mote doxxeo gual "

Vemos los t√©rminos m√°s frecuentes de varios de estos *clusters*:

![](misc/top_3_clusters_from_hate_words_word_freq_1.png)
![](misc/top_3_clusters_from_hate_words_word_freq_2.png)
![](misc/top_3_clusters_from_hate_words_word_freq_3.png)


Al igual que como se observ√≥ en la secci√≥n anterior, el ordenamiento por frecuencia no muestra una separaci√≥n clara entre las palabras de odio/agresi√≥n y las que no.

Vemos ahora los t√©rminos ordenados por informaci√≥n mutua puntual

![](misc/top_3_clusters_from_hate_words_word_pmi_1.png)
![](misc/top_3_clusters_from_hate_words_word_pmi_2.png)
![](misc/top_3_clusters_from_hate_words_word_pmi_3.png)

Vemos que las palabras se separan mejor; no obstante, se aprecia que las palabras de odio ordenadas seg√∫n su PMI se parecen a las palabras encontradas al aplicar este mismo criterio en la secci√≥n anterior.

### 6.4. An√°lisis cercano de dos *clusters*

En particular, se seleccionaron dos *clusters* que nos resultaron de inter√©s que no estaban categorizados seg√∫n los dos criterios tomados en las secciones anteriores, los etiquetamos manualmente como de odio/agresivos, y evaluamos cu√°l es el rendimiento del modelo sobre los mismos.

Los *clusters* seleccionados fueron el de g√©nero (99) y el de soberan√≠a (94). Para cada caso, se realiz√≥ un etiquetado a mano de cada comentario, respecto a si el mismo conten√≠a discurso de odio y contenido agresivo. Esto se hizo con el fin de poder analizar la calidad de la detecci√≥n del modelo en estos casos particulares.

A modo de aclaraci√≥n, el etiquetado de ambos *clusters* se realiz√≥ seg√∫n el criterio de quienes hicimos este trabajo; el mismo fue est√° sujeto a errores u omisiones. No obstante, consideramos que resulta muy importante para poder obtener una vista del rendimiento del modelo, y de sus puntos fuertes y d√©biles.

Los comentarios de estos *clusters* con etiquetado manual se encuentran en los siguientes documentos:

- [An√°lisis manual de cluster de g√©nero](/src/docs/analisis/genero.csv).
- [An√°lisis manual de cluster de soberan√≠a](/src/docs/analisis/soberania.csv).

A continuaci√≥n, vemos los resultados de las predicciones de cada *cluster*:

#### *Cluster* de G√©nero

El *cluster* 99 contiene comentarios que hacen referencia a temas de g√©nero, tales como: "mujer, hombre, no binario, homosexual, trans", entre otros.

Vemos la distribuci√≥n de las palabras del *cluster* seg√∫n su frecuencia e informaci√≥n mutua:

![](misc/genero_freq.png)

![](misc/genero_pmi.png)

Varios aspectos a mencionar:
* El puntaje promedio es mayor cuando los comentarios se predicen como de odio/agresivos.
* Las nubes de palabras que ordenan los t√©rminos por frecuencia expresa mucho mejor los comentarios de este t√≥pico que la que los ordena por informaci√≥n mutua.
* Adem√°s, en la nube de palabras clasificadas como de odio, las palabras que mayor PMI tienen son considerablemente distintas a las que se observaron en las secciones anteriores.

Vemos ahora la matriz de confusi√≥n del modelo al realizar predicciones en este *cluster*:

![](misc/confusion_matrix_genero.png)

Como se puede observar, se cuenta con un conjunto de datos en donde la mayor√≠a no son de odio (100, frente a 27), y se distribuyen de forma similar los errores, tanto la cantidad de falsos positivos como de falsos negativos.

Vemos algunos ejemplos de predicciones del modelo:

Predichos correctamente como discurso de odio / agresivos:

- "Vamos todos juntos!!: "*a La mUjEr sE le CrEe sieMpReEEe!!!*""
- "Seguro era un hombre vestido de mujer!!! las mujeres no hacen esas cosas, son seres de luz! jamas harian eso!!!"
- "Espert es lo mejor que hay, lamentablemente nunca va a llegar a ser presidente porque su mujer es fea.. A menos que se separe y establezca relaci√≥n con una mujer m√°s atractiva."
- "Pero los hombres son pajeros y lo hacen gratis. Conseguir hombres es casi gratis."


Predichos incorrectamente como discurso de odio / agresivos:

- "Es cierto que las c√°rceles de mujeres son mucho peores que las de los hombres?"
- "Pobre hombre. Pobre familia. Ni se lo vio venir ):"
- "Ajajja escribo re contra mal, pero es cierto que puede afectar a los hombres! Graciassss"
- "La pregunta para definir si ir es: aparte de lo que contas, hab√≠a Mujeres?"


Predichos correctamente como no discurso de odio / agresivos:

- "Est√°s minimizando el sufrimiento de la mujer"
- "No binario quiere decir que no se identifica ni como mujer ni como hombre. Si se identifica como mujer entonces es binario."
- "Que el ministerio se llame "de mujeres y g√©neros" no es redundante?"
- "Uff siendo mujer debe ser mucho m√°s jodido‚Ä¶"


Predichos incorrectamente como no discurso de odio / agresivos:

- "Recuerden chiques: si al crimen lo comete una mujer, lo justificamos como sea. MAL"
- "Hombre y mujer, el resto son diferentes gamas de homosexualidad"
- "Si un hombre siquiera est√° cerca dd una mujer sin su completa aprobaci√≥n, es autom√°ticamente violencia de g√©nero, machismo y patriarcado.. - alguna feminazi."
- "Eso prueba que las mujeres siempre estan cachondas."


#### *Cluster* de Soberan√≠a

Este *cluster* (n√∫mero 94) incluye comentarios que hacen referencia a diferentes tipos de soberan√≠a, como la territorial. Dentro del t√≥pico se ven comentarios referidos al conflicto por el territorio Mapuche, comentarios sobre las Islas Malvinas, la aprobaci√≥n del Senado de la Naci√≥n de la Ley que establece el "D√≠a Nacional del Kimchi", entre muchos otros.

Vemos la distribuci√≥n de las palabras del *cluster* seg√∫n su frecuencia e informaci√≥n mutua:

![](misc/soberania_freq.png)
![](misc/soberania_pmi.png)

* El ordenamiento por frecuencia refleja mejor el t√≥pico de soberan√≠a que se habla, aunque el ordenamiento por PMI tambi√©n muestra algunos aspectos del t√≥pico.
* En este *cluster* en particular, la proporci√≥n de comentarios predichos como de odio por el modelo es muy baja; tambi√©n se da que tanto el puntaje como la cantidad de comentarios recibidos es menor para aquellos comentarios predichos como de odio que aquellos que no.

Vemos la matriz de confusi√≥n del modelo en este *cluster*:

![](misc/confusion_matrix_soberania.png)

* Al igual que en el *cluster* de g√©nero, la proporci√≥n de comentarios cuya etiqueta es de odio o agresivo es mayor (477) a aquellos que no (136).
* No obstante, la cantidad de comentarios correctamente predichos como de odio es muy baja (12), y la mayor√≠a de errores fueron, para nuestra sorpresa, falsos negativos (124, frente a 26 falsos positivos).
* Consideramos que esto se da porque muchos de los comentarios que consideramos como de odio al realizar el etiquetado manual, usan un lenguaje peyorativo sobre una minor√≠a (los mapuches), lo cu√°l dista bastante de los t√©rminos usados en comentarios con los que el modelo fue entrenado (MeOffendMex).

Vemos ahora algunas predicciones en concreto: Comentarios predichos correctamente como discurso de odio o agresivos:

- "No, ni siquiera. Esperan que el Estado los proteja mediante DDDH. Esto esta apuntado en contra del "empresario usurpador capitalista" y la gente v√≠ctima de los ataques de estos insurgentes terroristas de mierda."
- "Madre de terroristas ofrece ayuda a terroristas.. Mas noticias, el pronostico y un par de culos luego de los comerciales."
- "Los chilenos ya tienen a los """mapuches"""" en sus calles prendiendo fuego todo y algunos se metieron al congreso, dudo que puedan o tengan la intencion de hacer algo."
- "y de paso hacerte unos ntfs con la cara del mapuche flogger m√°rtir preso en chile para recaudar unos d√≥larcitos m√°s"


Predichos incorrectamente como discurso de odio o agresivos:

- "Lo de China no le perdono, porque siguiendo su logica no deberiamos ni estar negociando con EEUU por su "Moral". En los negocios internacionales no hay moral solo utilitarismo"
- "oh Rallo eres un tesoro nacional"
- "Igual eso no cambia la realidad del abandono de las islas y el descuido del pais"
- "Si es por tu cuenta no es natural"


Predichos correctamente como no discurso de odio / agresivos:

- "Ni en Corea tienen dia nacional del kimchi me parece"
- "Y Dolarizando te tenes que comer la impresi√≥n de billetes a lo argentina que est√° haciendo la reserva federal con este nuevo gobierno‚Ä¶ sin ninguno de los ‚Äúbeneficios‚Äù."
- "Listo para ir a las Malvinas (?)"
- "Cerca de la Patagonia est√° la Ant√°rtida."


Predichos incorrectamente como no discurso de odio / agresivos:

- "Gracias por la info dia a dia! Es importante estar al corriente de los atentados de los terroristas mapuches!. HAGA PATRIA ..."
- "Hay que ajusticiar a todos los emo mapuches."
- "Reprimir no, a esta altura tiene que ser balas de plomo"
- "Aqu√≠ vemos a dos machos de la especie paquerus peronistus luchando por marcar territorio"


### 6.5. Visualizaci√≥n de t√©rminos cercanos seleccionados

Para la siguiente visualizaci√≥n, tomamos varias palabras vistas hasta aqu√≠ en cada uno de los *clusters*, obtenidas por frecuencia o informaci√≥n mutua, y otras que se han ido probando, y vemos d√≥nde se situar√≠an las mismas seg√∫n su cercan√≠a a cada *cluster*, y cu√°les ser√≠an las palabras m√°s cercanas a la misma en el espacio proyectado, tanto para Word2vec como de fastText.
La motivaci√≥n detr√°s de este an√°lisis es que pueden descubrirse palabras de odio/agresivas a partir de otras.

Vemos las palabras m√°s cercanas a cada una de las distintas palabras en Word2vec, la distancia de cada una, y el *cluster* en el que ser√≠an clasificadas.

![](misc/cherrypicking_word2vec.png)

Vemos ahora las cercan√≠as de las distintas palabras en fastText

![](misc/cherrypicking_fasttext.png)

Tras observar las palabras similares a cada una de las otras tanto con Word2vec como con fastText, puede verse que: 
* En Word2vec, en algunos casos se obtienen t√©rminos muy representantivos, sea por contener palabras con significado parecido, o por manifestar el contexto donde ocurren las palabras, mientras que en otros no se observa a simple vista una relaci√≥n evidente. Una de las palabras que manifiesta su uso com√∫n en comentarios que suelen involucrar discursos de odio, es la palabra "Brian", donde se observa a trav√©s de sus t√©rminos relacionados, que ese nombre es usado de forma muy peyorativa, com√∫nmente en mensajes que contienen aporofobia.

* En fastText, se detectan mejor las mismas variantes de una misma palabra. Por ejemplo, "conurbano" se relaciona con la palabra peyorativa "congourbano" *[sic]* (usada en varios mensajes con discurso de odio), as√≠ como con "conurbanense" o "urbano", aunque de la misma manera, tambi√©n se relaciona con palabras con pronunciaci√≥n parecida pero significado totalmente distinto, como "conadu".


## Conclusiones

En este trabajo, se usaron t√©cnicas tanto de aprendizaje supervisado como de no supervisado, con el prop√≥sito de encontrar manifestaciones de discursos de odio, y los distintos sub-lenguajes usados en tales contextos.

Combinando ambos tipos de t√©cnicas, se valid√≥ que es posible realizar una detecci√≥n autom√°tica de palabras y formas de comunicaci√≥n asociadas con discursos de odio o agresividad, identificando aquellos t√≥picos en donde mayormente se utilizan, y los contextos y formas alternativas en la que se manifiestan.

En la parte de *clustering*, los m√©todos que mejor dieron resultado fueron los de *embeddings* neuronales (Word2vec y fastText); empleando los mismos es donde se pudo identificar mejor los t√≥picos y sublenguajes referentes a los mismos.
De ambos, el que mejor result√≥ para esta categorizaci√≥n fue el modelo entrenado con Word2vec, ya que capturaba mejor las palabras distintas pero con significado similar; fastText por otro lado, captur√≥ mejor las variantes de una misma palabra, pero muchas veces un t√≥pico estaba dominado espec√≠ficamente por una palabra y sus variantes.

Respecto a la predicci√≥n con un modelo, se emplearon modelos de aprendizaje supervisado entrenados con *datasets* que no eran necesariamente de detecci√≥n de discursos de odio, y que estaban realizados por comunidades con distintas formas de comunicarse (mayormente conformadas por usuarios espa√±oles y mexicanos), y de distintas plataformas (Twitter y p√°ginas de noticias/foros).
Pese a ello, tales modelos resultaron muy provechosos para detectar manifiestaciones de agresividad/odio en el contexto de r/argentina.

Por √∫ltimo, tras etiquetar manualmente datos de dos *clusters* seleccionados y realizar predicciones sobre ellos, observamos que la detecci√≥n de discursos de odio est√° atada a los sub-lenguajes usados en la comunidad (por ejemplo, algunos t√©rminos peyorativos contra minor√≠as s√≥lo se utilizan entre usuarios de ciertas comunidades de Argentina), y podr√≠a mejorar considerablemente si se incorporan algunos datos etiquetados al entrenamiento del modelo de entrenamiento.

Finalmente, la conclusi√≥n a la que llegamos tras realizar este trabajo, es que es totalmente provechoso avanzar en el uso de m√©todos autom√°ticos para detectar y caracterizar discursos de odio en sus distintas variantes, y que hay mucho margen para seguir aplicando miner√≠a de texto en pos de poder mitigar su impacto.
En la secci√≥n siguiente, se listan varias propuestas de trabajo futuro para realizar a partir de este trabajo.

## Trabajo futuro

### General

- Tomando el enfoque de este trabajo como base, buscar caracterizar el discurso de odio en otras comunidades de foros populares argentinos, tales como [Taringa!](https://www.taringa.net/), [r/republicaargentina](https://www.reddit.com/r/RepublicaArgentina/), [r/dankargentina](https://www.reddit.com/r/dankargentina/), o comunidades argentinas en Twitter.

- Explorar la relaci√≥n entre "baits" y la generaci√≥n de discursos de odio en los comentarios alrededor de los mismos. Por ejemplo, *posts* con informaci√≥n no verificada o con una editorializaci√≥n marcada (pudiendo estar generada tanto por un medio, o que el t√≠tulo haya sido cambiado por quien realiz√≥ el *post*), o memes o chistes con animosidad hacia un determinado grupo o persona.

### Clustering

- Usar coeficientes de silueta para determinar el n√∫mero √≥ptimo de *clusters*.

### Modelo

- Realizar optimizaci√≥n de h√≠per-par√°metros para mejorar el rendimiento de los modelos. Por ejemplo, probar distintos tama√±os de ventana en el entrenamiento de Word2vec, o distintas cantidades de estimadores en random forest.
  
- Realizar un etiquetado en diferentes comentarios de r/argentina que pertenezcan a ciertos *clusters* que potencialmente contengan odio (o bien que pertenezcan a un cierto *flair*), y entrenar un modelo a partir de ellos, para poder mejorar la detecci√≥n de comentarios de odio.

- Incorporar en el an√°lisis de los resultados en el *notebook* 6 a los distintos *datasets* modelos que se emplearon, como random forest o fastText, en los tres *datasets*, y ver cu√°les son los puntos de coincidencia y de divergencia de los mismos.


### Informaci√≥n de contexto

- Incorporar info de la comunidad, para ver qu√© tan de acuerdo estuvieron los usuarios con los comentarios.
  
- Incorporar el contexto del comentario padre, especialmente si se est√° respondiendo. Esto es dado que un mensaje puede no ser un mensaje de odio por s√≠ s√≥lo, pero s√≠ lo es al observar el comentario al que se contesta.
  
- Incorporar el puntaje y premios de los *posts* y comentarios en el an√°lisis.
  
- Considerar dejar de alguna forma los emojis, ya que tambi√©n pueden representar una forma de manifestar odio.
  
- Incorporar los *flairs* al an√°lisis, como por ejemplo: ‚Äú\[Serio\]‚Äù.
  
- Incluir en el contexto el an√°lisis morfosint√°ctico de las palabras.


## Fuentes consultadas para el trabajo

### Discursos de odio

- https://en.wikipedia.org/wiki/Hate_speech
- https://www.rightsforpeace.org/hate-speech
- https://www.un.org/en/genocideprevention/hate-speech-strategy.shtml
- https://fsi.stanford.edu/news/reddit-hate-speech
- https://variety.com/2020/digital/news/reddit-bans-hate-speech-groups-removes-2000-subreddits-donald-trump-1234692898
- https://www.cfr.org/backgrounder/hate-speech-social-media-global-comparisons
- https://www.reddithelp.com/hc/en-us/articles/360045715951-Promoting-Hate-Based-on-Identity-or-Vulnerability

### reddit API

- https://www.jcchouinard.com/reddit-api/


### Procesamiento de lenguaje natural

- Foundations of Statistical Natural Language Processing - Manning & Sch√ºtze (1999)
- https://spacy.io
- https://radimrehurek.com/gensim/
- https://www.nltk.org
- https://www.baeldung.com/cs/ml-word2vec-topic-modeling
- https://www.kdnuggets.com/2018/04/robust-word2vec-models-gensim.html
- https://adrian-rdz.github.io/NLP_word2vec/
- https://towardsdatascience.com/applying-machine-learning-to-classify-an-unsupervised-text-document-e7bb6265f52
- https://dylancastillo.co/nlp-snippets-cluster-documents-using-word2vec/
- https://www.roelpeters.be/calculating-mutual-information-in-python/

### Clustering

- https://towardsdatascience.com/k-means-clustering-8e1e64c1561c
- https://paperperweek.wordpress.com/2018/04/09/best-ways-to-cluster-word2vec/
- https://ai.intelligentonlinetools.com/ml/k-means-clustering-example-word2vec/
- https://medium.com/@rohithramesh1991/unsupervised-text-clustering-using-natural-language-processing-nlp-1a8bc18b048d
- https://xplordat.com/2018/12/14/want-to-cluster-text-try-custom-word-embeddings/
- https://towardsdatascience.com/clustering-with-more-than-two-features-try-this-to-explain-your-findings-b053007d680a


### Competencias

- HatEval (SemEval 2019): https://competitions.codalab.org/competitions/19935
- DETOXIS (IberLEF 2021): https://detoxisiberlef.wixsite.com/website/corpus
- MeOffendEs (IberLEF 2021): https://competitions.codalab.org/competitions/28679


### Trabajos relacionados

- https://github.com/jfreddypuentes/spanlp
- https://medium.com/ml2vec/using-word2vec-to-analyze-reddit-comments-28945d8cee57
- https://www.kaggle.com/szymonjanowski/internet-articles-data-with-users-engagement
- https://towardsdatascience.com/religion-on-twitter-5f7b84062304
- https://becominghuman.ai/detecting-gender-based-hate-speech-in-spanish-with-natural-language-processing-cdbba6ec2f8b
- https://www.learndatasci.com/tutorials/sentiment-analysis-reddit-headlines-pythons-nltk/
