

# Vistazo r√°pido

El presente repo contiene el c√≥digo correspondiente al proyecto final de la materia [Miner√≠a de datos para texto](https://sites.google.com/unc.edu.ar/textmining2021/), a cargo de [Laura Alonso i Alemany](https://cs.famaf.unc.edu.ar/~laura/).

Objetivo del proyecto: Caracterizar discursos de odio dentro de la comunidad de [reddit Argentina](https://reddit.com/r/argentina). Esto es, detectarlos y encontrar sub-lenguajes de odio en los mismos.

Para realizar esto, se llev√≥ a cabo un proceso consistente en 6 etapas, como se muestra en la siguiente figura:

![pipeline_reddit](/misc/workflow.drawio.png)


Cada etapa tiene su correspondiente notebook:

1. Obtenci√≥n del conjunto de comentarios de a trav√©s de la API de Reddit ([link](https://github.com/PerseoSoft/redditHateSpeech/blob/main/src/1_pipeline_download_reddit_comments.ipynb)).
   
2. Pre-procesamiento del mismo ([link](https://github.com/PerseoSoft/redditHateSpeech/blob/main/src/2_pipeline_preprocessing.ipynb)).

3. Aplicaci√≥n de embeddings y categorizaci√≥n en clusters (notebook [LDA](https://github.com/PerseoSoft/redditHateSpeech/blob/main/src/3a_pipeline_lda.ipynb) [Word2vec](https://github.com/PerseoSoft/redditHateSpeech/blob/main/src/3b_pipeline_embedding_word2vec.ipynb) [fastText](https://github.com/PerseoSoft/redditHateSpeech/blob/main/src/3c_pipeline_embedding_fasttext.ipynb)).

4. Entrenamiento de un modelo de detecci√≥n de odio y extracci√≥n de palabras de odio en cada dataset ([link](https://github.com/PerseoSoft/redditHateSpeech/blob/main/src/4_detect_hate_speech.ipynb)).
Para realizar el entrenamiento de los modelos, es necesario contar con los datasets respectivos de tres competencias (Hateval, DETOXIS, MeOffendMex) que se desee entrenar.

5. Uso del modelo para predecir los comentarios recolectados ([link](https://github.com/PerseoSoft/redditHateSpeech/blob/main/src/5_pipeline_hate_speech.ipynb)).

6. Combinaci√≥n de dicho modelo con las categor√≠as encontradas para encontrar correlaciones ([link](https://github.com/PerseoSoft/redditHateSpeech/blob/main/src/6_pipeline_result.ipynb)).

**Este informe y proyecto est√°n en proceso üößüî®, todav√≠a sujetos a cambios, correcciones, y mejoras**


## Instalaci√≥n

### Instalaci√≥n con conda

Instalar Anaconda ([ver aqu√≠](https://docs.anaconda.com/anaconda/install/index.html)) y luego ejecutar:

```bash
#Crear entorno con conda y activarlo
conda env create -f environment.yml
conda activate hateSpeech
#Descarga del Trained pipelines de spaCy
python -m spacy download es_core_news_lg
#Correr Jupyter Lab
jupyter lab --ip=0.0.0.0 --no-browser --allow-root --NotebookApp.token='' --NotebookApp.password=''
```
Ir a [http://localhost:8888](http://localhost:8888) para acceder a la UI de Jupyter.

### Instalaci√≥n con Docker Compose

Instalar Docker Compose ([ver aqu√≠](https://docs.docker.com/compose/install/)) y luego ejecutar:

```bash
#Construir imagen
docker-compose build
#Correr Jupyter Lab
docker-compose up -d
```

Ir a [http://localhost:8888](http://localhost:8888) para acceder a la UI de Jupyter.

## Flujo de datos generados

Los distintos notebooks forman un pipeline en el cu√°l cada uno utiliza los datos generados por el anterior. Se listan cada una de las entradas:

1. Obtenci√≥n de comentarios. 
    - Archivos de entrada: N/A. 
    - Archivo de salida: *docs/reddit_data.csv*: CSV que contiene los comentarios de reddit descargados

2. Pre-procesamiento del dataset.
    - Archivos de entrada: *docs/reddit_data.csv*.
    - Archivos de salida: *docs/preprocessing_reddit_data.csv*: CSV con los comentarios pre-procesados.
   

3. Embeddings y clustering.
    - Archivos de entrada: *docs/preprocessing_reddit_data.csv*.
    - Archivos de salida: 
      - *docs/reddit_data_METODO.csv*, donde *METODO* puede ser 'lda', o 'word2vec', 'fasttext'. Cada uno de estos archivos toma el dataset pre-procesado y le agrega el n√∫mero de cl√∫ster al que pertenecer√≠a cada comentario, seg√∫n su cercan√≠a.
      - *docs/models/MODEL.model*, el modelo entrenado. Puede ser 'word2vec', o 'fasttext'. 
      - *docs/models/MODEL_kmeans.model*, el modelo de k-means entrenado usando los embeddings de *MODEL* (para 'word2vec' y 'fasttext').

    
4. Entrenamiento y selecci√≥n del modelo.
   - Archivos de entrada: *docs/hateval2019/hateval2019_es_train.csv*, *docs/detoxis_data/train.csv*, y *docs/MeOffendEs/mx-train-data-non-contextual.csv*. Estos archivos requieren la descarga previa manual de cada dataset.
   - Archivos de salida: para cada dataset, se guarda:
     - Palabras de odio de cada modelo: *docs/palabras_odio.csv*.
     - Vectorizador: *docs/models/DATASET_vectorizer.pkl* donde *DATASET* es hateval, detoxis, o meoffendmex.
     - Modelo entrenado: *docs/models/DATASET_INICIALES_MODELO_model.pkl* donde *INICIALES_MODELO* es 'lr', 'rf', o 'nb'.
   - Archivos de salida (de prueba): Predicciones: *docs/test/reddit_DATASET_hate_comments.csv*, uno para cada *DATASET*: 'hateval', 'detoxis', 'meoffendmex'.
   
5. Aplicaci√≥n del modelo en comentarios de reddit. 
   - Archivos de entrada: *docs/reddit_data_METODO.csv*.
   - Archivos de salida:
     - *docs/reddit_data_hate_speech.csv* - CSV que toma  **TODO**
6. An√°lisis de resultados.
   - Archivos de entrada: *docs/reddit_data_hate_speech.csv*
   - Archivos de salida: N/A.


---

# Informe del proyecto

Se muestra a continuaci√≥n el informe producto de este proyecto, en donde se especifican la motivaci√≥n y objetivos del trabajo, y los distintos enfoques abordados para realizar la detecci√≥n de odio.

√çndice

- [Vistazo r√°pido](#vistazo-r√°pido)
  - [Instalaci√≥n](#instalaci√≥n)
    - [Instalaci√≥n con conda](#instalaci√≥n-con-conda)
    - [Instalaci√≥n con Docker Compose](#instalaci√≥n-con-docker-compose)
  - [Flujo de datos generados](#flujo-de-datos-generados)
- [Informe del proyecto](#informe-del-proyecto)
  - [Introducci√≥n](#introducci√≥n)
    - [Discursos de odio](#discursos-de-odio)
    - [Motivaci√≥n del trabajo](#motivaci√≥n-del-trabajo)
    - [reddit](#reddit)
      - [¬øPor qu√© r/argentina?](#por-qu√©-rargentina)
  - [Paso a paso del proyecto](#paso-a-paso-del-proyecto)
    - [1. Obtenci√≥n de los datos](#1-obtenci√≥n-de-los-datos)
    - [2. Pre-procesamiento](#2-pre-procesamiento)
    - [3. Representaci√≥n de t√≥picos mediante embeddings](#3-representaci√≥n-de-t√≥picos-mediante-embeddings)
      - [3a. Embeddings con LDA](#3a-embeddings-con-lda)
    - [3b. Embeddings con Word2vec](#3b-embeddings-con-word2vec)
    - [3c. Embeddings con fastText](#3c-embeddings-con-fasttext)
  - [4. Entrenamiento del detector de odio](#4-entrenamiento-del-detector-de-odio)
  - [5. Aplicaci√≥n del modelo a los comentarios de reddit](#5-aplicaci√≥n-del-modelo-a-los-comentarios-de-reddit)
  - [6. An√°lisis de resultados](#6-an√°lisis-de-resultados)
    - [Vista general de los distintos clusters](#vista-general-de-los-distintos-clusters)
    - [Vista de los clusters con mayor proporci√≥n de predicci√≥n positiva](#vista-de-los-clusters-con-mayor-proporci√≥n-de-predicci√≥n-positiva)
    - [](#)
    - [An√°lisis detallado de dos cl√∫sters](#an√°lisis-detallado-de-dos-cl√∫sters)
      - [Cluster de G√©nero](#cluster-de-g√©nero)
      - [Cluster de Soberan√≠a](#cluster-de-soberan√≠a)
  - [Conclusiones](#conclusiones)
  - [Trabajo futuro](#trabajo-futuro)
    - [General](#general)
    - [Clustering](#clustering)
    - [Modelo](#modelo)
    - [Informaci√≥n de contexto](#informaci√≥n-de-contexto)
  - [Fuentes consultadas para el trabajo](#fuentes-consultadas-para-el-trabajo)
    - [Discursos de odio](#discursos-de-odio-1)
    - [reddit API](#reddit-api)
    - [Procesamiento de lenguaje natural](#procesamiento-de-lenguaje-natural)
    - [Clustering](#clustering-1)
    - [Competencias](#competencias)
    - [Trabajos relacionados](#trabajos-relacionados)


## Introducci√≥n

### Discursos de odio

El discurso de odio es un problema muy relevante en la actualidad, dado su rol en la discriminaci√≥n de grupos y minor√≠as sociales, y [es considerado como precursor de cr√≠menes de odio, que incluyen al genocidio](). **TODO agregar cita**

Hay varias posturas sobre lo que es el discurso de odio, en general se coincide en que es un discurso que:

1. Apunta contra un grupo o individuo, basado en alg√∫n aspecto como su orientaci√≥n sexual, religi√≥n, nacionalidad, etc.
2. Busca humillar, discriminar o propagar el odio/hostilidad/intolerancia hacia ese grupo.
3. Tiene una intenci√≥n deliberada.

Su manifestaci√≥n en Internet, adem√°s:

1. Puede motivar formas de agresi√≥n en l√≠nea.
2. Permite propagar el discurso de odio con velocidad.
3. Permite que el discurso se mantenga y comparta con facilidad.
4. Facilita la generaci√≥n de c√°maras de eco.
5. Al estar en servidores privados, la aplicaci√≥n de la ley no siempre es r√°pida, lo que hace que ciertos actores intenten eludir su control, utilizando el discurso de odio en beneficio de su agenda.

A ra√≠z de la gravedad que significa el problema, muchas plataformas sociales han reconocido el problema, y han optado por prohibirlo en sus t√©rminos de uso, pudiendo sus usuarios reportar comentarios que potencialmente contengan este tipo de discursos. **TODO citar**
No obstante, el problema de la propagaci√≥n de odio permanece...
 **TODO citar**


### Motivaci√≥n del trabajo

Considerando las consecuencias que pueden traer aparejadas los discursos de odio, este trabajo se enfoca en la detecci√≥n de tales discursos en una comunidad particular de reddit. Los objetivos del mismo son: **1)** detecci√≥n de comentarios con discurso de odio y **2)** caracterizar ese discurso de odio en sub-lenguajes de odio.

El presente trabajo se basa en la siguiente hip√≥tesis: *"en una comunidad en donde existen comentarios con discurso de odio, es beneficioso combinar t√©cnicas de aprendizaje supervisado y no supervisado, para realizar la detecci√≥n de subcomunidades de odio, a partir de modelos que se especializan en distintos grupos de comentarios"*.

### reddit

[Reddit](https://www.reddit.com/) es una red social de ‚Äúcomunidades‚Äù, creadas y moderadas por sus propios usuarios. En cada comunidad, sus miembros hacen posts, y cada post puede ser comentado generando debate. Su aspecto distintivo es que cada post o comentario recibe votos, con el objetivo de que aquellos posts o comentarios que m√°s aportan aparezcan encima de los que no. Tambi√©n se pueden premiar a aquellos destacados. 

En la siguiente imagen podemos ver la estructura general de un post en reddit:

![](misc/reddit.png)


En este proyecto, nos centramos en [r/argentina](https://www.reddit.com/r/argentina/), que es una comunidad dedicada a charlar temas referentes a Argentina, que incluyen comidas, costumbres, chistes, deporte, pol√≠tica,  econom√≠a, consejos, entre otros.

#### ¬øPor qu√© r/argentina?

Quisimos hacer nuestro trabajo enfocado en una comunidad Argentina fuera de las redes sociales m√°s comunes (dado que son aquellas m√°s frecuentemente estudiadas), pero que a la vez tenga el tama√±o suficiente como para tener muchos usuarios e interacciones. En ese sentido, r/argentina fue la opci√≥n m√°s prominente, ya que la comunidad es muy activa y cuenta con cerca de 350.000 suscriptores (a Noviembre de 2021).

Respecto a su posici√≥n frente a discursos de odio, en las reglas de r/argentina (en concreto, la Regla 3) se deja totalmente de manifiesto su prohibici√≥n. Citando textualmente:

>**3. No se permite el racismo, xenofobia u otras expresiones de odio**
>
> No se permite el racismo, xenofobia, ni ninguna otra forma de odio (incluyendo sexismo, homofobia, transfobia, clase social, etc), ni ning√∫n tipo de discriminaci√≥n o expresiones de odio o lenguaje deshumanizante en general; esto incluye comentarios incitando violencia. Esto tambi√©n se extiende a grupos. Hacer referencia a enfermedades o discapacidades para insultar a otros no ser√° tolerado. Usuarios que incurran en estas faltas podr√°n ser baneados permanentemente sin apelaci√≥n.


No obstante, al elaborar este trabajo, hemos detectado casos de comentarios con discursos de odio, ej.: manifestando [aporofobia](https://es.wikipedia.org/wiki/Aporofobia), [obesofobia](https://es.wikipedia.org/wiki/Obesofobia), o comentarios agresivos contra mujeres, entre otros.

Dada esta situaci√≥n, la motivaci√≥n de nuestro trabajo es la de poder detectar autom√°ticamente este tipo de comentarios, pudiendo caracterizar los mismos en sub-comunidades.


## Paso a paso del proyecto

Se describe a continuaci√≥n, el paso a paso de las distintas etapas de este proyecto, partiendo de los datos iniciales, c√≥mo los mismos fueron procesados y usados para entrenar distintos algoritmos, los resultados obtenidos tras ello, y finalmente las conclusiones y trabajo futuro.


### 1. Obtenci√≥n de los datos

[Notebook](/src/1_pipeline_download_reddit_comments.ipynb)

Para la obtenci√≥n de los datos se utiliz√≥ un *wrapper* de la API de reddit, llamado [PRAW](https://praw.readthedocs.io/en/stable/index.html), a partir del cual se descargaron comentarios de diferentes *post* del r/argentina, as√≠ como las respuestas de los comentarios.
Los posts en reddit pueden ser de tipo *link* (por ejemplo, colocando el link hacia una noticia), o pueden ser de tipo texto.
Para la descarga de comentarios de cada post, se consideraron s√≥lo aquellos que conten√≠an texto, y una cierta cantidad de caracteres como m√≠nimo.

De cada comentario que se guard√≥ de reddit, se obtuvieron los siguientes datos:
- *id*: identificador del *post* o comentario. Guardado por cuestiones de trazabilidad.
- *comment_parent_id*: identificador del comentario al cu√°l responde el comentario actual, en caso que corresponda. Se guard√≥ por cuestiones de trazabilidad.
- *flair*: categor√≠a del post, asignada por el usuario que lo crea (a partir de una lista brindada por el propio subreddit). En el caso de r/argentina, las categor√≠as incluyen t√≥picos como "Pol√≠tica", "Econom√≠a", "Humor", "Historia" o "Serio".
- *comms_num*: n√∫mero de respuestas que recibi√≥ el comentario.
- *score*: es un puntaje que los usuarios le dieron al comentario.

En total, se descargaron **TODO** comentarios, desde el d√≠a **TODO** hasta el **TODO**.


### 2. Pre-procesamiento

[Notebook](/src/2_pipeline_preprocessing.ipynb)

Teniendo descargados los datos, se aplic√≥ un pre-procesamiento sobre cada comentario, que consisti√≥ en:

- Eliminar emojis, urls, comillas, caracteres especiales y puntuaciones.
- Aplicar tokenizaci√≥n, dividiendo cada comentario en sus correspondientes palabras.
- Conversi√≥n a min√∫scula.
- Eliminaci√≥n de *stopwords* (mediante spaCy).
- Lematizaci√≥n (mediante spaCy).
- Construir bigramas y trigramas.

### 3. Representaci√≥n de t√≥picos mediante embeddings

Teniendo los comentarios pre-procesados, el siguiente objetivo fue detectar t√≥picos a partir de los mismos de acuerdo a las co-ocurrencias de las palabras, para poder identificar los distintos temas que se hablan, y los sublenguajes empleados en ellos.

Para poder llevar esto a cabo, se emplearon tres m√©todos en los datos obtenidos:

1. Latent Dirichlet Allocation.
2. Word2vec.
3. fastText.

Se describe a continuaci√≥n cada uno de ellos, mostrando particularmente algunos comentarios que fueron agrupados a trav√©s de las diferentes t√©cnicas aplicadas. Un evento particular que sucedi√≥ durante la descarga de estos datos en reddit fue el debate de la "[Ley de Promoci√≥n de la Alimentaci√≥n Saludable](https://www.boletinoficial.gob.ar/detalleAviso/primera/252728/20211112)", tambi√©n conocida como "ley de etiquetado frontal". Vamos a comparar las subcomunidades obtenidas en cada t√©cnica, analizando particularmente aqu√©llas referidas a este evento.

**TODO agregar los t√≥picos detectados de insultos / posible odio para comparar en los tres modelos**

#### 3a. Embeddings con LDA

[Notebook](/src/3a_pipeline_lda.ipynb)

El primer modelo que se comenz√≥ utilizando es [Latent Dirichlet Allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation), que es un m√©todo generativo que asume que cada documento est√° compuesto por una mezcla de t√≥picos, y donde cada palabra tiene una probabilidad de relacionarse con cada uno de ellos.
La elecci√≥n inicial de LDA se fundament√≥ en que es un m√©todo s√≥lido para detecci√≥n de t√≥picos en corpus de texto.

El modelo se aplic√≥ probando tama√±os de cl√∫sters de 30 a 120, y distintas configuraciones de h√≠per-par√°metros. No obstante, los resultados obtenidos  no fueron satisfactorios, ya que a la hora de realizar un an√°lisis de los t√≥picos identificados por el modelo, se encontr√≥ poca cohesi√≥n entre los t√≥picos detectados.

En la siguiente imagen se pueden observar algunos de los t√≥picos identificados por LDA.

![](misc/embedding_1.png)

El t√≥pico n√∫mero 91, **piedra - etiqueta - pan - mira**, incluye comentarios sobre la tratativa de la ley de etiquetado y temas que tienen que ver con la comida en general. Algunos comentarios son:

1. "Me alegro mucho, seguro muy feliz todos por el reencuentro. Igual te recomiendo que no coma directo de la lata, pasale a un platito o comedero. Entiendo que a veces ni te dan tiempo."
2. "Todo mi secundario el desayuno fue un fantoche triple y una lata de coca.  Y s√≥lo gastaba 2. Qu√© buenos tiempos."
3. "La manteca no hace mal. Es muy dif√≠cil comer exceso de grasas para tu cuerpo en comparaci√≥n con lo f√°cil que es atiborrarte con az√∫car y carbohidratos. Esos son los verdaderos enemigos"
4. "Y con etiquetas que te dicen cu√°nta grasa tiene un kilo de bayonesa"
5. "Alta banfest se van a mandar los mods con este thread. Despedite de tu cuenta, maquinola, denunciado"


**TODO agregar tambi√©n una imagen de las proyecciones realizadas con PCA (que ser√≠an los embeddings aqu√≠ aplicados) para mostrar la distribuci√≥n de t√≥picos. Tambi√©n se puede agregar m√°s info sobre h√≠per-par√°metros y dem√°s**


### 3b. Embeddings con Word2vec

[Notebook](/src/3b_pipeline_embedding_word2vec.ipynb)

Dado que el funcionamiento con LDA no se consider√≥ como satisfactorio, el siguiente paso consisti√≥ probar otro tipo de modelos: los *embeddings* de palabras.
Los mismos consisten en llevar las palabras a un nuevo espacio, de forma tal que aquellas que comparten un contexto com√∫n en los comentarios obtenidos, tiendan a encontrarse mucho m√°s cerca que aquellas que no.
De esta manera, se podr√≠an identificar subcomunidades en este nuevo espacio.

Para ello, se llevaron a cabo los siguientes pasos:

1. Entrenar el modelo de generaci√≥n de embeddings de palabras mediante una *tarea de pretexto* (dada una palabra, predecir informaci√≥n relacionada a su contexto, por ejemplo una palabra que le sigue). Se emplearon dos modelos: [Word2vec](https://en.wikipedia.org/wiki/Word2vec), cuyos resultados se muestran en esta secci√≥n, y [fastText](https://en.wikipedia.org/wiki/fastText), mostrado en la siguiente.
2. Una vez entrenados los modelos, se procedi√≥ a generar una representaci√≥n vectorial de cada comentario, donde cada uno se mape√≥ a un vector num√©rico de acuerdo al promedio de los embeddings de cada una de sus palabras.
3. Se aplic√≥ el algoritmo de *clustering* *[k-means](https://en.wikipedia.org/wiki/K-means_clustering)*, tomando los vectores generados en el paso anterior.

Tras realizar el entrenamiento y aplicar clustering, se observaron que los t√≥picos obtenidos se identificaban de forma mucho mejor que al usar LDA.
En la siguiente imagen se pueden observar algunas de las subcomunidades identificadas tras aplicar Word2vec.

**TODO mencionar que se probaron distintos numeros de cl√∫sters y el que mejor funcion√≥ fue 120, porque se identifican claramente ciertos t√≥picos, a pesar de que otros no tienen una identidad com√∫n**

![](misc/embedding_2.png)

En particular, el *cluster* n√∫mero 94, **ley - etiquetado - proyecto**, es el que incluye comentarios sobre la tratativa de la ley de etiquetado y temas que tienen que ver con las leyes en general. Algunos comentarios del mismo son:

1. "Una prueba mas de la ley de oferta y demanda"
2. "Con la nueva ley no le pod√©s regalar leche entera o un alfajor a un comedor, decir comida basura en un pa√≠s donde el 50\% de los chicos no hacen toda las comidas es lo m√°s clasista que existe."
3. "Recuerden la ley de alquileres.... Fu√© sancionada con un beso muy fuerte de los K, PRO y dem√°s muchachos..."
4. "No entiendo c√≥mo hay tanta gente en contra de una ley que no te cambia un carajo tu vida. Es la ley m√°s anodina que sac√≥ el Kirchnerismo en toda su historia creo"
5. "Pero hay leyes contra la violencia de genero! Como paso esto!!!1!?"
6. "No existe tal cosa en Argentina. Existe el Estado de Sitio, pero no se asemeja para nada a una ley marcial.. El concepto de ley marcial como tal, desapareci√≥ en el 94 con la nueva Constituci√≥n."

### 3c. Embeddings con fastText

[Notebook](/src/3c_pipeline_embedding_fasttext.ipynb)

Finalmente, el √∫ltimo m√©todo aplicado fue [fastText](https://en.wikipedia.org/wiki/fastText) que entrena una tarea de pretexto para generar un embedding de palabras al igual que Word2vec, pero adem√°s tiene en cuenta las sub-palabras, lo cu√°l resulta √∫til para identificar las alteraciones que puede tener una misma palabra.

En la siguiente imagen se pueden observar algunas de las subcomunidades identificadas por fastText.

![](misc/embedding_3.png)

Como se puede ver en el cluster **jaja - jajaja - jajajar - jajajaja - jajaj**, fastText identifica mejor las alteraciones que pueden suceder dentro de una palabra.

El *cluster* n√∫mero 113, **ley - etiquetado - votar**, incluye comentarios sobre la tratativa de la ley de etiquetado y temas que tienen que ver con las leyes en general. Algunos comentarios son:

1. "Feriado con fines tur√≠sticos. Ley 27.399"
2. "ajajaja como los cagaron a los primeros. como siempre la ley aplica a todos por igual /s"
3. "El sticker en Chile fue durante la transici√≥n de la ley. Imag√≠nate tener productos fabricados y tener que cambiar la envoltura a todos para que cumplan la ley"
4. "Gracias gloriosa ley de regulaci√≥n de alimentos, ahora se que desayunar coca cola con surtidos bagleys esta mal"
5. "Eso y que la ley va a prohibir vender dulces y gaseosas en los colegios, y usar im√°genes de famosos en los envases."
6. "Eso est√° por la ley Micaela no?. Tipo esta clase de capacitaciones no?"
7. "y ahora Lipovetzky reconoce lo de la ley de alquileres"

Si bien existen algunos *clusters* que nos permiten identificar t√≥picos especificos (como el 113), se observ√≥ que si bien el m√©todo detecta variantes de palabras, en t√©rminos generales los *clusters* no se traducen en t√≥picos cohesivos. Por ejemplo, en el *cluster* n√∫mero 54 encontrarmos comentarios de diferentes t√≥picos:
1. "No lo veo a Belgrano? Saavedra?. Me re mintieron!"
2. "Mate de cafe re copado, un litro de cafe en tu organismo"
3. "ajajajajajaj Geologia, es re linda carrera igual pero esta materia es una completa mierda"
4. "cuando dije eso? milei est√° re bajon desde el debate del otro dia, me lo dice gente que habla con el casi todos los dias"

Tambi√©n se observ√≥ que algunos *clusters* se construyen exclusivamente alrededor de una palabra y sus variantes, por ejemplo el 43 se construy√≥ alrededor de la palabra **decir**:

1. "Por eso dije ""en general"". Hay excepciones."
2. "Son los muy menos. Yo dir√≠a que 1 de cada 100."
3. "6! Seis! Seis, por favor! Dije seissss??!!"
4. "sera lo que el gobierno diga"
5. "Lo s√© lo s√©... Me lo dec√≠a mi abuela"

Observando esto, y el buen rendimiento observado al usar Word2vec, se opt√≥ finalmente por avanzar en la identificaci√≥n de subcomunidades empleando dicha t√©cnica.


## 4. Entrenamiento del detector de odio

[Notebook](/src/4_detect_hate_speech.ipynb)


En paralelo a la b√∫squeda de cl√∫sters que agrupan los distintos t√≥picos, se busc√≥ tambi√©n, a partir de los datos [pre-procesados anteriormente](#2-pre-procesamiento) el detectar autom√°ticamente comentarios de odio, para poder combinarlos con los [t√≥picos encontrados](#3-embeddings). Para ello, se recurri√≥ a conjuntos de datos anotados y en castellano, que hayan utilizados para tareas similares. En particular, se opt√≥ por los siguientes tres:

**TODO poner las etiquetas que se decidieron usar en cada dataset**

1. HatEval: dataset con cerca de 7000 tweets de usuarios de Espa√±a, que potencialmente manifiestan discurso de odio contra mujeres o inmigrantes. Este dataset es el m√°s parecido a la tarea que queremos resolver, ya que tiene datos etiquetados que marcan directamente si se trata o no de un tweet con discurso de odio, sea contra un individuo o un grupo. Ejemplo de comentario etiquetado como discurso de odio: **TODO** Ejemplo de comentario sin etiqueta de discurso de odio: **TODO**

2. DETOXIS: dataset con cerca de 3500 comentarios de sitios de noticias/foros espa√±oles, que posiblemente contienen toxicidad. Si bien un mensaje con toxicidad no es necesariamente discurso de odio (y un mensaje con discurso de odio puede tener toxicidad o no), suele estar asociado al mismo. Ejemplo de comentario t√≥xico sin discurso de odio: **TODO** Ejemplo de comentario t√≥xico con discurso de odio: **TODO**

3. MeOffendMex: dataset con alrededor de 5000 tweets de usuarios de M√©xico, que posiblemente contienen mensajes ofensivos. Al igual que la toxicidad, un mensaje ofensivo no necesariamente est√° manifestando odio, pero suelen estar asociados. Ejemplo de comentario ofensivo con discurso de odio: **TODO** Ejemplo de comentario ofensivo sin discurso de odio: **TODO**

En cada uno de los mismos, se entrenaron tres modelos de aprendizaje supervisado: *[regresi√≥n log√≠stica](https://en.wikipedia.org/wiki/Logistic_regression)*, *[naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)* y *[random forest](https://en.wikipedia.org/wiki/Random_forests)*, todos provistos por la librer√≠a [scikit-learn](https://scikit-learn.org).

Para realizar el entrenamiento, a cada comentario se le aplic√≥ el vectorizador [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), que transform√≥ cada comentario en una matriz *sparse* de forma

**TODO**

$$X = $$

donde los predictores representan los unigramas, bigramas y trigramas de cada comentario.

Tal matriz, junto con las correspondientes etiquetas de cada comentario, constituyeron la entrada de cada uno de los modelos. Tales modelos funcionaron bastante bien con sus configuraciones b√°sicas, **TODO**, mostrando matrices de confusi√≥n s√≥lidas. Especialmente, los que mejor performaron fueron naive Bayes y random forest.

Una vez entrenados, se extrajeron las palabras que posiblemente manifiestan odio en cada dataset, en base al entrenamiento de los modelos de naive Bayes y random forest, de acuerdo a su aporte a la clasificaci√≥n de las palabras **TODO**.

**TODO agregar matrices de confusi√≥n, y comentar un poco los criterios tomados, especialmente respecto a los falsos positivos**


La salida del detector de odio se puede ver en el archivo **TODO**.


## 5. Aplicaci√≥n del modelo a los comentarios de reddit

[Notebook](/src/5_pipeline_hate_speech.ipynb)

Una vez teniendo los modelos entrenados, el siguiente paso consisti√≥ en aplicarlos en los comentarios recolectados de reddit.

Al aplicar los modelos entrenados en los comentarios, lo primero que se observ√≥ es la cantidad de falsos positivos detectados como comentario de odio.

En particular, el dataset cuyo mejor rendimiento observamos detectando comentarios en reddit fue MeOffendEs **TODO**. A partir de esto, se guardaron los resultados y **TODO**.

Los modelos entrenados detectaron .


## 6. An√°lisis de resultados

[Notebook](/src/6_pipeline_result.ipynb)

En la siguiente secci√≥n, se toman los [clusters generados](#3-representaci√≥n-de-t√≥picos-mediante-embeddings), los [modelos entrenados](#4-entrenamiento-del-detector-de-odio) y [sus predicciones](#5-aplicaci√≥n-del-modelo-a-los-comentarios-de-reddit), para llevar a cabo un an√°lisis de los resultados obtenidos.

Para este an√°lisis, se us√≥ el modelo entrenado con Naive Bayes (con un umbral de 0.8) sobre el conjunto de datos MeOffendMex, y el modelo Word2vec entrenado previamente.

### Vista general de los distintos clusters

Vemos una vista general de los datos con los que se cuenta hasta ahora, con respecto a su distribuci√≥n en los distintos clusters.

* Se cuenta con 27.791 comentarios, donde cada uno tiene asignado un n√∫mero de t√≥pico y una etiqueta indicando si el clasificador lo categoriz√≥ como discurso de odio / agresivo o no. Los mensajes se distribuyen en los t√≥picos de la siguiente manera:

![](misc/num_topicos.png)

* En muchos de los clusters se identifican t√≥picos concretos. Algunos ejemplos:
  * Cluster 8: econom√≠a.
  * Cluster 18: pol√≠tica.
  * Cluster 23: d√≥lar.
  * Cluster 94: leyes.
  * Cluster 98: comidas.
  * Cluster 99: g√©nero.
  * Cluster 116: insultos.

* De los 27.791 comentarios, 2075 fueron predichos como de odio por el clasificador. Tales prediciones de distribuyen como sigue:

![](misc/pred_hs_por_topico.png)

* De los distintos cl√∫sters, existen varios cuyo porcentaje de comentarios predicho como odio es muy significativo:

| N√∫mero de cluster | \% pred. positivas |
|:-----------------:|:-----------------------:|
|        116        |           73\%          |
|         66        |           39\%          |
|         79        |           36\%          |
|         27        |           27\%          |
|         93        |           24\%          |


* Vemos tambi√©n el porcentaje de comentarios predichos en cada flair:

|     Flair    | \% pred. positivas |
|:------------:|:-----------------------:|
|  Historiaüá¶üá∑  |         11\%        |
|  Policialesüö® |         10\%        |
|   Pol√≠ticaüèõÔ∏è  |         9\%        |
|   Meet-up‚ùó   |         9\%        |
|    VideoüìΩÔ∏è    |         9\%        |


* Vemos, para todos los cl√∫sters en general y para los tres de mayor proporci√≥n de predicciones en particular, si existe una correlaci√≥n lineal tanto entre el puntaje y la cantidad de r√©plicas de cada comentario, y su predicci√≥n como mensaje de odio.

| Cluster | Corr. puntaje y pred. pos. | Corr. num. com. y pred. pos. |
|:-------:|:--------------------------:|:----------------------------:|
|  Todos  |          -0.001         |           -0.016          |
|   116   |          0.028          |           0.005           |
|    66   |          0.068          |           0.170           |
|    79   |          -0.025          |           -0.150          |

Se puede observar al ver todos los cl√∫sters, que no existe una correlaci√≥n lineal entre puntaje o cantidad de comentarios obtenidos y clasificaci√≥n o no como discurso de odio. Por otra parte, al ver esto en los tres cl√∫sters donde mayor proporci√≥n de discurso de odio se detect√≥, se observa que la correlaci√≥n var√≠a levemente seg√∫n el caso.


### Vista de los clusters con mayor proporci√≥n de predicci√≥n positiva

Vemos ahora las palabras de mayor frecuencia (tanto predichas o no como odio), encontradas en los tres cl√∫sters con m√°s proporci√≥n de predicciones positivas (el 116, 66 y 79).

![](misc/top_3_clusters_word_freq_1.png)
![](misc/top_3_clusters_word_freq_2.png)
![](misc/top_3_clusters_word_freq_3.png)

Puede observarse que se detectan muchos insultos en los tres cl√∫sters. No obstante, no se distingue una separaci√≥n clara de los t√©rminos usados (tanto de odio como de no odio) al realizar agrupamiento por t√©rminos m√°s frecuentes. Por ello, se opt√≥ por ordenarlos seg√∫n su [informaci√≥n mutua puntual](https://es.wikipedia.org/wiki/Punto_de_informaci%C3%B3n_mutua). Se muestra abajo como quedar√≠an entonces los t√©rminos agrupados de esta forma, en donde se puede ver que el ordenamiento es mucho mejor:

![](misc/top_3_clusters_word_pmi_1.png)
![](misc/top_3_clusters_word_pmi_2.png)
![](misc/top_3_clusters_word_pmi_3.png)

### 


**TODO comentar sobre la combinaci√≥n entre enfoque autom√°tico y manual, especialmente con esto √∫ltimo que seleccionamos dos cl√∫sters en particular**

**comentar sobre el enfoque tomado con discurso de odio/agresiones**

**incluir tambien las cercan√≠as de las palabras de odio sacadas de los modelos a otros cl√∫sters**

### An√°lisis detallado de dos cl√∫sters

En particular, se seleccionaron dos clusters que nos resultaron de inter√©s, para hacer una vista m√°s cercana, y evaluar concretamente cu√°l es el rendimiento del modelo, frente a un etiquetado manual realizado en ambos.

Los clusters a analizar fueron el de g√©nero (99) y soberan√≠a (94). Para cada caso, se realiz√≥ un etiquetado a mano de cada comentario, respecto a si el mismo conten√≠a discurso de odio, y si el mismo ten√≠a un contenido agresivo. Esto se hizo con el fin de poder analizar la calidad de la detecci√≥n del modelo en casos particulares. El criterio tomado fue el siguiente: dada .

Aclaraci√≥n: el etiquetado de ambos clusters fue realizado seg√∫n el criterio de quienes hicimos este trabajo; el mismo fue hecho seg√∫n nuestros propios criterios, y est√° sujeta a errores u omisiones. No obstante, consideramos que resulta muy importante para poder obtener una vista del rendimiento del modelo, de sus puntos fuertes y d√©biles.

Los comentarios de estos clusters con etiquetado manual se encuentran en los siguientes documentos:

- [An√°lisis manual de cluster de g√©nero](/src/docs/analisis/genero.csv).
- [An√°lisis manual de cluster de soberan√≠a](/src/docs/analisis/soberania.csv).

A continuaci√≥n, vemos los resultados del an√°lisis de cada cluster:

#### Cluster de G√©nero

El *cluster* 99 contiene comentarios que hacen referencia a temas de g√©nero, tales como: "mujer, hombre, no binario, homosexual, trans", entre otros.

Vemos la distribuci√≥n de las palabras del cluster seg√∫n su frecuencia e informaci√≥n mutua:

![](misc/genero_freq.png)

![](misc/genero_pmi.png)

Vemos ahora las m√©tricas del modelo en este cluster:



**TODO**

Vemos algunos ejemplos de predicciones del modelo:

**TODO agregar la comparaci√≥n entre lo realizado por el modelo y lo manual.**

Predichos correctamente como discurso de odio / agresivos:

- "Vamos todos juntos!!: "*a La mUjEr sE le CrEe sieMpReEEe!!!*""
- "Seguro era un hombre vestido de mujer!!! las mujeres no hacen esas cosas, son seres de luz! jamas harian eso!!!"
- "Espert es lo mejor que hay, lamentablemente nunca va a llegar a ser presidente porque su mujer es fea.. A menos que se separe y establezca relaci√≥n con una mujer m√°s atractiva."
- "Pero los hombres son pajeros y lo hacen gratis. Conseguir hombres es casi gratis."


Predichos incorrectamente como discurso de odio / agresivos:

- "Es cierto que las c√°rceles de mujeres son mucho peores que las de los hombres?"
- "Pobre hombre. Pobre familia. Ni se lo vio venir ):"
- "Ajajja escribo re contra mal, pero es cierto que puede afectar a los hombres! Graciassss"
- "La pregunta para definir si ir es: aparte de lo que contas, hab√≠a Mujeres?"


Predichos correctamente como no discurso de odio / agresivos:

- "Est√°s minimizando el sufrimiento de la mujer"
- "No binario quiere decir que no se identifica ni como mujer ni como hombre. Si se identifica como mujer entonces es binario."
- "Que el ministerio se llame "de mujeres y g√©neros" no es redundante?"
- "Uff siendo mujer debe ser mucho m√°s jodido‚Ä¶"


Predichos incorrectamente como no discurso de odio / agresivos:

- "Recuerden chiques: si al crimen lo comete una mujer, lo justificamos como sea. MAL"
- "Hombre y mujer, el resto son diferentes gamas de homosexualidad"
- "Si un hombre siquiera est√° cerca dd una mujer sin su completa aprobaci√≥n, es autom√°ticamente violencia de g√©nero, machismo y patriarcado.. - alguna feminazi."
- "Eso prueba que las mujeres siempre estan cachondas."

**TODO Analizar el promedio de los puntajes y comentarios. Se puede ver que los hate speech tienen m√°s pruntajes y n√∫mero de comentarios que lo que no lo son**


#### Cluster de Soberan√≠a

Este cluster (n√∫mero 94) incluye comentarios que hacen referencia a diferentes tipos de soberan√≠a, como la territorial. Dentro del t√≥pico se ven comentarios referidos al conflicto por el territorio Mapuche, a comentarios sobre las Islas Malvinas, la aprobaci√≥n del Senado de la Naci√≥n de la Ley que establece el "D√≠a Nacional del Kimchi", entre muchos otros.

Vemos la distribuci√≥n de las palabras del cluster seg√∫n su frecuencia e informaci√≥n mutua:

![](misc/soberania_freq.png)
![](misc/soberania_pmi.png)



Predichos correctamente como discurso de odio o agresivos:

- No, ni siquiera. Esperan que el Estado los proteja mediante DDDH. Esto esta apuntado en contra del "empresario usurpador capitalista" y la gente v√≠ctima de los ataques de estos insurgentes terroristas de mierda.
- Madre de terroristas ofrece ayuda a terroristas.. Mas noticias, el pronostico y un par de culos luego de los comerciales.
- Los chilenos ya tienen a los """mapuches"""" en sus calles prendiendo fuego todo y algunos se metieron al congreso, dudo que puedan o tengan la intencion de hacer algo.
- y de paso hacerte unos ntfs con la cara del mapuche flogger m√°rtir preso en chile para recaudar unos d√≥larcitos m√°s


Predichos incorrectamente como discurso de odio o agresivos:

- Lo de China no le perdono, porque siguiendo su logica no deberiamos ni estar negociando con EEUU por su "Moral". En los negocios internacionales no hay moral solo utilitarismo
- oh Rallo eres un tesoro nacional
- Igual eso no cambia la realidad del abandono de las islas y el descuido del pais
- Si es por tu cuenta no es natural


Predichos correctamente como no discurso de odio / agresivos:

- Ni en Corea tienen dia nacional del kimchi me parece
- Y Dolarizando te tenes que comer la impresi√≥n de billetes a lo argentina que est√° haciendo la reserva federal con este nuevo gobierno‚Ä¶ sin ninguno de los ‚Äúbeneficios‚Äù.
- Listo para ir a las Malvinas (?)
- Cerca de la Patagonia est√° la Ant√°rtida.


Predichos incorrectamente como no discurso de odio / agresivos:

- Gracias por la info dia a dia! Es importante estar al corriente de los atentados de los terroristas mapuches!. HAGA PATRIA ...
- Hay que ajusticiar a todos los emo mapuches.
- Reprimir no, a esta altura tiene que ser balas de plomo
- Aqu√≠ vemos a dos machos de la especie paquerus peronistus luchando por marcar territorio



## Conclusiones

- .

## Trabajo futuro

### General

- Tomando el enfoque de este trabajo como base, buscar caracterizar el discurso de odio en otras comunidades de foros populares argentinos, tales como [Taringa!](https://www.taringa.net/), [r/republicaargentina](https://www.reddit.com/r/RepublicaArgentina/), [r/dankargentina](https://www.reddit.com/r/dankargentina/), o comunidades argentinas en Twitter.

- Explorar la relaci√≥n entre "baits" y la generaci√≥n de discursos de odio en los comentarios alrededor de los mismos. Por ejemplo, posts con informaci√≥n no verificada o con una editorializaci√≥n marcada (pudiendo estar generada tanto por un medio, o que el t√≠tulo haya sido cambiado por quien realiz√≥ el post), o memes o chistes con animosidad hacia un determinado grupo o persona.

### Clustering

- Usar coeficientes de silueta para determinar el n√∫mero √≥ptimo de cl√∫sters.

### Modelo

- Realizar optimizaci√≥n de h√≠per-par√°metros para mejorar el rendimiento de los modelos.
  
- Realizar un etiquetado en diferentes comentarios de r/argentina que pertenezcan a ciertos clusters que potencialmente contengan odio (o bien que pertenezcan a un cierto Flair), y entrenar un modelo a partir de ellos, para poder mejorar la detecci√≥n de comentarios de odio.

### Informaci√≥n de contexto

- Incorporar info de la comunidad, para ver qu√© tan de acuerdo estuvieron los usuarios con los comentarios.
  
- Incorporar el contexto del comentario padre, especialmente si se lo est√° respondiendo. Esto es dado que un mensaje puede no ser un mensaje de odio por s√≠ s√≥lo, pero s√≠ lo es al observar el comentario al que se contesta.
  
- Incorporar el puntaje y premios de los posts y comentarios en el an√°lisis.
  
- Considerar dejar de alguna forma los emojis, ya que tambi√©n pueden representar una forma de manifestar odio.
  
- Incorporar los tags al an√°lisis, como por ejemplo: ‚Äú\[Serio\]‚Äù.
  
- Incluir en el contexto el an√°lisis morfosint√°ctico de las palabras.


## Fuentes consultadas para el trabajo

### Discursos de odio

- https://en.wikipedia.org/wiki/Hate_speech
- https://www.rightsforpeace.org/hate-speech
- https://www.un.org/en/genocideprevention/hate-speech-strategy.shtml
- https://fsi.stanford.edu/news/reddit-hate-speech
- https://variety.com/2020/digital/news/reddit-bans-hate-speech-groups-removes-2000-subreddits-donald-trump-1234692898
- https://www.reddithelp.com/hc/en-us/articles/360045715951-Promoting-Hate-Based-on-Identity-or-Vulnerability

### reddit API

- https://www.jcchouinard.com/reddit-api/


### Procesamiento de lenguaje natural

- Foundations of Statistical Natural Language Processing - Manning & Sch√ºtze (1999)
- https://spacy.io
- https://radimrehurek.com/gensim/
- https://www.nltk.org
- https://www.baeldung.com/cs/ml-word2vec-topic-modeling
- https://www.kdnuggets.com/2018/04/robust-word2vec-models-gensim.html
- https://adrian-rdz.github.io/NLP_word2vec/
- https://towardsdatascience.com/applying-machine-learning-to-classify-an-unsupervised-text-document-e7bb6265f52
- https://dylancastillo.co/nlp-snippets-cluster-documents-using-word2vec/
- https://www.roelpeters.be/calculating-mutual-information-in-python/

### Clustering

- https://towardsdatascience.com/k-means-clustering-8e1e64c1561c
- https://paperperweek.wordpress.com/2018/04/09/best-ways-to-cluster-word2vec/
- https://ai.intelligentonlinetools.com/ml/k-means-clustering-example-word2vec/
- https://medium.com/@rohithramesh1991/unsupervised-text-clustering-using-natural-language-processing-nlp-1a8bc18b048d
- https://xplordat.com/2018/12/14/want-to-cluster-text-try-custom-word-embeddings/
- https://towardsdatascience.com/clustering-with-more-than-two-features-try-this-to-explain-your-findings-b053007d680a


### Competencias

- HatEval (SemEval 2019): https://competitions.codalab.org/competitions/19935
- DETOXIS (IberLEF 2021): https://detoxisiberlef.wixsite.com/website/corpus
- MeOffendEs (IberLEF 2021): https://competitions.codalab.org/competitions/28679


### Trabajos relacionados

- https://github.com/jfreddypuentes/spanlp
- https://medium.com/ml2vec/using-word2vec-to-analyze-reddit-comments-28945d8cee57
- https://www.kaggle.com/szymonjanowski/internet-articles-data-with-users-engagement
- https://towardsdatascience.com/religion-on-twitter-5f7b84062304
- https://becominghuman.ai/detecting-gender-based-hate-speech-in-spanish-with-natural-language-processing-cdbba6ec2f8b
- https://www.learndatasci.com/tutorials/sentiment-analysis-reddit-headlines-pythons-nltk/
